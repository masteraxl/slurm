<html>
<head>
<title>SLURM Administrator's Guide</title>
</head>
<body>
<h1>SLURM Administrator's Guide</h1>
<h2>Overview</h2>
Simple Linux Utility for Resource Management (SLURM) is an open source,
fault-tolerant, and highly scalable cluster management and job 
scheduling system for Linux clusters of 
thousands of nodes.  Components include machine status, partition
management, job management, and scheduling modules.  The design also 
includes a scalable, general-purpose communication infrastructure.
SLURM requires no kernel modifications and is relatively self-contained. 
There is a "control" machine, which orchestrates SLURM activies. A "backup 
controller" is also desirable to assume control functions in the event 
of a failure in the control machine. There are also compute servers on 
which applications execute, which can number in the thousands.

<h2>Configuration</h2>
There are three SLURM configuration files that you need to establish: 
overall SLURM options, node configurations, and partition configuration. 
The overall SLURM configuration options indicate where to find the 
other configuration files, where to find the daemons, how often to 
perform certain actions, etc. The node configuration tell SLURM 
what nodes it is to manage as well as their anticipated hardware 
and system software configurations. The partition configuration 
permits you to establish different job limits or access lists for 
various groups (or partitions) of nodes. 

<h3>Overall SLURM Configuration</h3>
The overall SLURM configuration is provided by the file "/etc/SLURM.conf".
This file identifies names for administrator names, time limits, configuration 
file names, key resource names, etc. This file or a copy of it must execute 
on every computer under SLURM management. The following paramters may be specified:
<dl>
<dt>Administrators
<dd>A comma list of users permitted to execute SLURM administrative commands 
(the default value is "root"). 

<dt>ControlMachine
<dd>The name of the machine where SLURM control functions are executed 
(e.g. "lx01"). This value must be specified.

<dt>BackupController
<dd>The name of the machine where SLURM control functions are to be 
executed in the event that ControlMachine fails (e.g. "lx02"). This node
may be used as a compute server by default. It will come into service 
as a controller only upon the failure of ControlMachine and will revert 
to a "standby" mode when the ControlMachine becomes available once again. 
While not essential, it is highly recommended that you specify a backup 
controller.

<dt>NodeSpecConf
<dd>Fully qualified pathname of the file containing node configuration information 
as described below (the default value is "/usr/local/SLURM/NodeSpecConf"). This 
file must be accessible to the ControlMachine and BackupController.

<dt>PartitionConf
<dd>Fully qualified pathname of the file containing partition configuration 
information as described below (the default value is  "/usr/local/SLURM/PartitionConf").
This file must be accessible to the ControlMachine and BackupController.

<dt>MasterDaemon
<dd>The fully qualified pathname of the file containing the SLURM daemon
to execute on all machines, execute InitProgram then execute ControlDaemon 
and/or ServerDaemon (the default value is "/usr/local/SLURM/Slurmd.Master").
This file must be accessible to all machines. <i>Do we want to use this model 
or create different RC configuration file for the different machine types?</i>

<dt>InitProgram
<dd>This program must execute and return an exit code of zero before the 
ControlDaemon or ServerDaemon enter into service. This would normally be 
used to insure that the computer is fully ready for executing user jobs. 
It may, for example, wait until every required file system has been mounted.
By default there is no initialization program.

<dt>ControlDaemon
<dd>The fully qualified pathname of the file containing the SLURM daemon
to execute on the ControlMachine (the default value is "/usr/local/SLURM/Slurmd.Control").
This file must be accessible to the ControlMachine and BackupController.

<dt>ServerDaemon
<dd>The fully qualified pathname of the file containing the SLURM daemon
to execute on every compute server node (the default value is "/usr/local/SLURM/Slurmd.Server").
This file must be accessible to every SLURM compute server.

<dt>Prolog
<dd>This program is executed on each node allocated to a job prior to its initiation. 
By default no prolog is executed. 

<dt>Epilog
<dd>This program is executed on each node allocated to a job after its termination. 
This can be used to remove temporary files created by the job or other clean-up.
By default no epilog is executed. 

<dt>HeartbeatInterval
<dd>How frequently the ServerDaemon should report its state to the ControllerDaemon. 
Also, how frequently the ControllerDaemon should report its state to the BackupController. 
The default value is 60 and the units are seconds.

<dt>ControllerTimeout
<dd>How long to wait for the ControllerDaemon to respond before assuming it 
has failed and starting the BackupController. The default value is 300 
and the units are seconds.

<dt>ServerTimeout
<dd>How long to wait for the ServerDaemon to respond before assuming it 
has failed and declaring the node DOWN then terminating any job running on 
it. The default value is 300 and the units are seconds.

<dt>PrimaryBackupInterval
<dd>How long to wait between making primary SLURM state saves. The default 
value is -1 (not taken) and the units are seconds.

<dt>SecondaryBackupInterval
<dd>How long to wait between making secondary SLURM state saves. The default 
value is -1 (not taken) and the units are seconds.

<dt>PrimaryNodeBackup
<dd>The fully qualified pathname of the file where the primary SLURM node 
state information is saved. There is no default value. The file should 
be accessible to at least the ControlMachine and (ideally) also the BackupController.
Ideally the primary and secondary backup files will be made to distinct file 
systems and/or devices for greater fault tolerance. 
This is a text file that may be viewed and/or modified as required.

<dt>SecondaryNodeBackup
<dd>The fully qualified pathname of the file where the secondary SLURM node 
state information is saved. There is no default value. The file should 
be accessible to at least the ControlMachine and (ideally) also the BackupController.
This is a text file that may be viewed and/or modified as required.

<dt>PrimaryPartitionBackup
<dd>The fully qualified pathname of the file where the primary SLURM partition 
state information is saved. There is no default value. The file should 
be accessible to at least the ControlMachine and (ideally) also the BackupController.
This is a text file that may be viewed and/or modified as required.

<dt>SecondaryPartitionBackup
<dd>The fully qualified pathname of the file where the secondary SLURM partition 
state information is saved. There is no default value. The file should 
be accessible to at least the ControlMachine and (ideally) also the BackupController.
This is a text file that may be viewed and/or modified as required.

<dt>PrimaryJobBackup
<dd>The fully qualified pathname of the file where the primary SLURM job 
state information is saved. There is no default value. The file should 
be accessible to at least the ControlMachine and (ideally) also the BackupController.
This is a text file that may be viewed and/or modified as required.

<dt>SecondaryJobBackup
<dd>The fully qualified pathname of the file where the secondary SLURM job 
state information is saved. There is no default value. The file should 
be accessible to at least the ControlMachine and (ideally) also the BackupController.
This is a text file that may be viewed and/or modified as required.
</dl>

<p>
Lines in the configuration file having "#" in column one will be 
considered comments.
The configuration file should contain one keyword followed by an 
equal sign, followed by the value (one keyword per line). 
In the interest of simplicity (for the developers), the field 
descriptors above are case sensitive. 
The size of each line in the file is limited to 1024 characters.
A <a href="#SampleOver">SLURM configuration file</a> is included at the end of this document.

<h3>Node Configuration</h3>
The node configuration permits you to identify the nodes (or machines) 
to be managed by SLURM. You may identify the hardware and/or software 
characteristics of the node in the configuration file. SLURM operates 
in a heterogeneous environment and users are able to specify resource 
requirements to achieve the desired scheduling characteristics. 
Note that some of these values are not appropriate for you to 
set and this will be described in detail below.
This file must be accessible to both the ControlMachine and 
BackupController.
The node configuration file contains the following information: 
<dl>

<dt>Name
<dd>Name of a node as returned by hostname (e.g. "lx12").

<dt>IP
<dd>The IP addess of the node to be used for communications 
(e.g. "123.4.56.789").

<dt>Partition
<dd>List of partition numbers this node belongs to, partition numbers 
range from 0 to 31 and are specified with comma separators (e.g. "1,3").
This can be altered by resetting MAX_PARTITION in the slurm.h file 
before building. In no case should this value exceed the number of 
bits in an integer on the computer as a bit-mask is used to record 
partition information. The default partition value is zero.

<dt>OS
<dd>Operating System name and level (output of the command
"/bin/uname -s -r | /bin/sed 's/ /./g'", e.g. "Linux.2.4.7-10"). 
The default value is "UNKNOWN"

<dt>CPUs
<dd>Number of processors on the node (e.g. "2"). The default 
value is 1.

<dt>Speed
<dd>Relative speed of these processors. Units can be an arbitrary 
floating point number, but MHz value is recommended (e.g. "863.8").
The default value is 1.

<dt>RealMemory
<dd>Size of real memory on the node in MegaBytes (e.g. "2048").
The default value is 1.

<dt>VirtualMemory
<dd>Size of virtual memory on the node in MegaBytes (e.g. "4096").
The default value is 1.

<dt>TmpDisk
<dd>Total size of temporary disk storage on "/tmp" in MegaBytes 
(e.g. "16384"). Note this does not indicate the amount of free 
space available to the user on the node, only the total file 
system size. The system administration should insure this file 
system is purged as needed so that user jobs have access to 
most of this space. The default value is 1.

<dt>FreeDisk
<dd>The amount of unallocated space on "/tmp" in MegaBytes 
(e.g. "16384"). <i>I can't imaging this being of much use, 
especially if purging is performed in a reasonable fashion. Comments?</i>

<dt>LastResponse
<dd>Time of last contact from node, format is time_t as returned 
by the "time" function. The default value is 0.

<dt>State
<dd>State of the node with respect to the initiation of user jobs. 
Acceptable values are "UNKNOWN", "IDLE", "BUSY", "DOWN", "DRAINED", 
"DRAINING". For example, the SLURM control machine may very well 
not be used for initiation of user jobs and its state would thus 
be set to "DOWN". The default value is "UNKNOWN".
</dl>

<p>
Only the Name must be supplied in the configuration file; all other 
items are optional.
If you operate with more than one partition, Partition should also 
be specified. 
Other configuration information can be established through communications 
with the SLURM Daemon, slurmd actually running on each node.
ALternately, you can explicitly establish baseline values in the 
configuration file. 
Nodes which register to the system with less than the configured resources 
(e.g. too little memory), will be placed in the "DRAINED" state to 
avoid scheduling jobs on them.
By default all nodes will be in partition zero, but it is possible 
to configure your system with multiple overlapping partitions (more 
on that below).
If a node is not to be included in any partition, indicate this with the 
expression "Partition= ".
Lines in the configuration file having "#" in column one will be 
considered comments.
The configuration file should contain information about one node on 
a single line. 
If more than one line is used to describe a node's configuration, 
be sure to include "Name=" on each line.
In the interest of simplicity (for the developers), the field 
descriptors above are case sensitive. 
Each field should contain the field's name, an equal sign, and the value. 
Fields should be space or tab separated.
The default values for each node can be specified with a record in which 
"Name" is "DEFAULT". 
The default entry values will apply only to lines following it in the 
configuration file and the default values can be reset multiple times 
in the configuration file with multiple entries where "Name=DEFAULT".
In order to support the concept of jobs requiring consecutive nodes, 
nodes should be place in this file in consecutive order.
The size of each line in the file is limited to 1024 characters.
A <a href="#SampleNode">sample node configuration file</a> is 
included at the end of this document.
<p>
The node states have the following meanings:
<dl>
<dt>UNKNOWN
<dd>Default initial node state upon startup of SLURM.
An attempt will be made to contact the node and get current state information.

<dt>IDLE
<dd>The node is idle and available for use.

<dt>BUSY
<dd>The node has been allocated work (a user job) and is processing it. 
SLURM is designed to allocate whole nodes to a job. Multiple parallel 
jobs are not initiated on individual nodes. 

<dt>DOWN
<dd>The node is unavailable for use. It has been configured as not being 
under SLURM control or failed to respond to system state inquiries or has 
explicitly removed itself from service due to a failure. This state 
typically indicates some problem requiring administrator intervention.

<dt>DRAINING
<dd>The node has been unavailble for new work by explicit administrator 
intervention. It is processing some work at present and will enter state
"DRAINED" when that work has been completed. This might be used to 
prepare some nodes for maintenance work.

<dt>DRAINED
<dd>The node is idle, but not available for use. Since this state is 
entered by explicit administrator request, additional administator 
intervention is typically not required.
</dl>
<p>
SLURM uses a hash table in order to locate table entries rapidly. 
Each table entry can be directly accessed without any searching
if the name contains a sequence number suffix. SLURM can be built 
with the HASH_MODE set to indicate the hashing mechanism. Possible 
values are "DECIMAL" and "OCTAL" for names containing sequence numbers 
or "OTHER" which processes mixed alpha-numeric without sequence numbers. 
If you use a naming convention lacking a sequence number, it may be 
desirable to review the hashing functions Hash_Index and Rehash in the 
Mach_Stat_Mgr.c module. This is especially important in clusters having 
large numbers of nodes. Non-numeric information in the name is 
ignored (if desired, they could differ for each node). The sequence 
numbers can start at zero, one, or any other desired number, but 
should contain consecutive numbers. The sequence number portion 
can contain leading zeros for a consistent name length, if so 
desired. For example, name nodes lx01, lx02, lx03, lx04, 
lx05, lx06, lx07, lx08, lx09, lx10, lx11, lx12, lx13, lx14, 
lx15, lx16 for excellent performance. Note that correct operation 
will be provided with any nodes names, but performance will suffer 
without this optimization.

<h3>Partition Configuration</h3>
The partition configuration permits you to establish different job 
limits or access lists for various groups (or partitions) of nodes. 
Nodes may be in more than one partition. This configuration file 
must be accessible to the SLURM controller and backup controller
machines. The partiton configuration file contains the following 
information: 
<dl>
<dt>Name
<dd>Name by which the partition may be referenced (e.g. "Interactive"). 
This name can be used by users when submitting their jobs.

<dt>Number
<dd>Unique number by which the partition can be referenced. This is 
used in the node configuration file.

<dt>JobType
<dd>Job types which may execute in the partition. Possible values 
are "BATCH", "INTERACTIVE", and "ALL". The default value is "ALL".

<dt>MaxTime
<dd>Maximum wall-time limit for any job in minutes. The default 
value is "UNLIMITED", which is represented internally as -1.

<dt>MaxCpus
<dd>Maximum count of CPUs which may be allocated to any single job,
The default value is "UNLIMITED", which is represented internally as -1.

<dt>State
<dd>State of partition or availability for use.  Possible values 
are "UP" or "DOWN". The default value is "UP".

<dt>AllowUsers
<dd>Names of user who may use the partition, separated by commas. 
The default value is "ALL". If AllowUsers is specified, then 
the value of DenyUsers will be ignored.

<dt>DenyUsers
<dd>Names of user who may not use the partition, separated by commas.
The  default value is "NONE".
</dl>

<p>
Only the first two items, Name and Number, must be supplied in the 
configuration file.
If not otherwise specified, all nodes will be in partition zero.
Lines in the configuration file having "#" in the first collumn 
will be considered comments.
It is recommended that configuration file contain information 
about one partition per line. If more than one line is used to 
describe the configuration of a partition, specify the "Name=" 
on each line.
In the interest of simplicity (for the developers), the field 
descriptors above are case sensitive. 
Each field should contain the field's name, an equal sign, and the value. 
Fields should be space or tab separated.
The default values for each partition can be specified with a record in which 
"Name" is "DEFAULT" if other default values are desired. 
The default entry values will apply only to lines following it in the 
configuration file and the default values can be reset multiple times 
in the configuration file with multiple entries where "Name=DEFAULT".
The size of each line in the file is limited to 1024 characters.
If user controls are desired then set either AllowUsers or DenyUsers, but not both.
If AllowUsers is set, then DenyUsers is ignored. 
A <a href="#SamplePart">sample partition configuration file</a> is included at the end of this document.

<h3>Job Configuration</h3>
The job configuration format specified below is used for the SLURM daemons 
to save state information. The format specified below is also used by the 
slurm_admin administration tool to modify job state information: 
<dl>

<dt>Number
<dd>Unique number by which the job can be referenced. This value 
may not be changed by slurm_admin.

<dt>Name
<dd>Name by which the job may be referenced (e.g. "Simulation"). 
This name can be specified by users when submitting their jobs.

<dt>JobType
<dd>Permitted job types values are "BATCH" and "INTERACTIVE". 

<dt>MaxTime
<dd>Maximum wall-time limit for the job in minutes. An "UNLIMITED"
value is represented internally as -1.

<dt>Nodes
<dd>Comma separated list of nodes which are allocated to the job. 
This value may not be changed by slurm_admin.

<dt>State
<dd>State of the job.  Possible values are "PENDING", "STARTING", 
"RUNNING", and "ENDING". 

<dt>User
<dd>Name of the user executing this job.
</dl>

<h2>Slurm_admin Administration Tool</h2>
The tool you will primarily use in the administration of SLURM is slurm_admin. 
It provides the means of viewing and updating node and partition 
configurations. It can also be used to update some job state 
information. You can execute slurm_admin with a single keyword on 
the execute line or it will query you for input and process those 
keywords on an interactive basis. The slurm_admin keywords are shown below.
A <a href="#SampleAdmin">sample slurm_admin session</a> with examples is appended.

<p>
Usage: slurm_admin [-q | -v] [&lt;keyword&gt;]<br>
-q is equivalent to the "quiet" keyword<br>
-v is equivalent to the "verbose" keyword<br>
</pre>
<dl>
<dt>exit
<dd>Terminate slurm_admin.

<dt>help
<dd>Display this list of slurm_admin commands and options.

<dt>quiet
<dd>Print no messages other than error messages.

<dt>quit
<dd>Terminate slurm_admin.

<dt>reconfigure [&lt;NodeName&gt;]
<dd>The SLURM daemons on the specified node are instructed to re-read 
the configuration files. The default is that all daemons on all nodes 
are reconfigured.

<dt>restart [&lt;NodeName&gt;]
<dd>The SLURM daemons on the specified node are stopped and restarted. 
The default is that all daemons on all nodes are stopped and restarted. 
All state information for the daemons are preserved.

<dt>show &lt;entity&gt; [&lt;ID&gt;]
<dd>Show the configuration for a given entity. Entity must 
be "job", "node", or "partition". By default, state information 
for all records is reported. If you only wish to see the 
state of one entity record, specify either its ID number
(assumed if entirely numberic) or its name. 

<dt>start [&lt;NodeName&gt;]
<dd>The SLURM daemons on the specified node are started as needed. 
The default is that all daemons on all nodes are started.

<dt>stop [&lt;NodeName&gt;]
<dd>The SLURM daemons on the specified node are stopped. 
The default is that all daemons on all nodes are stopped. 
All state information for the daemons are preserved for 
whenever the daemons are restarted. Note this will <b>not</b> 
terminate any job executing on the node. You may set a 
node's state to "DRAINING", set any running job's state to 
"ENDING", wait for the job to terminate, then stop the 
daemon on a node to leave no active jobs.

<dt>update &lt;entity&gt; &lt;options&gt;
<dd>Update the configuration for a given entity. Entity 
must be "job", "node", or "partition". Options are of the same 
format as the configuration files. 

<dt>upload [&lt;NodeName&gt;]
<dd>Upload into the SLURM node configuration table actual configuration 
as actually reported by the node (memory, CPU count, temporary disk, etc.).
This can be used to establish a baseline configuration rather than 
entering the configurations manually into a file. By default information 
from all nodes is uploaded. 

<dt>verbose
<dd>Enable detailed logging of slurm_admin execution state information.

<dt>version
<dd>Display the slurm_admin tool version number.

<dt>write &lt;entity&gt; &lt;filename&gt;
Write current entity configuration information to the specified file. 
Entity must be "job", "node", or "partition". 
</dl>

<h2>Miscellaneous</h2>
There is no necessity for synchronized clocks on the nodes. 
Events occur either in real-time based upon message traffic 
or based upon changes in the time on a node. However, synchronized 
clocks will permit easier analysis of SLURM logs from multiple 
nodes.
<p>
SLURM uses the syslog function to record events. It uses a 
range of importance levels for these messages. Be certain
that your system's syslog functionality is operational.

<a name="SampleOver"><h2>Sample SLURM Configuration File</h2></a>
<pre>
# 
# Sample /etc/SLURM.conf
# Author: John Doe
# Date: 11/06/2001
#
Administrators=cdunlap,garlick,grondo,jette
#
ControlMachine=lx01
BackupController=lx02
#
NodeSpecConf=/usr/local/SLURM/NodeSpecConf
PartitionConf=/usr/local/SLURM/PartitionConf
#
MasterDaemon=/usr/local/SLURM/Slurmd.Master"
InitProgram=/usr/local/SLURM/Slurmd.Prolog"
ControlDaemon=/usr/local/SLURM/Slurmd.Control"
ServerDaemon=/usr/local/SLURM/Slurmd.Server"
ControllerTimeout=120
ServerTimeout=90
</pre>


<a name="SampleNode"><h2>Sample Node Configuration File</h2></a>
<pre>
# 
# Sample /usr/local/SLURM/NodeSpecConf
# Author: John Doe
# Date: 11/06/2001
#
Name=DEFAULT OS=Linux.2.4.7-1 CPUs=16 Speed=345.0 RealMemory=2048 VirtualMemory=4096 TmpDisk=16384 State=IDLE
#
# lx01-lx02 for login and control functions only, node state is DRAINED for SLURM initiated jobs
Name=lx01 State=DRAINED
Name=lx02 State=DRAINED
#
# lx03-lx09 for partitions 1 (debug) and 3 (super)
Name=DEFAULT Partition=1,3
Name=lx03
Name=lx04
Name=lx05 
Name=lx06 
Name=lx07 TmpDisk=4096
Name=lx08 
Name=lx09 
#
# lx10-lx30 for partitions 0 (pbatch) and 3 (super)
Name=DEFAULT Partition=0,3
Name=lx10 
Name=lx11 VirtualMemory=2048
Name=lx12 RealMemory=1024 
Name=lx13 
Name=lx14 CPUs=32
Name=lx15 
Name=lx16 
Name=lx17 
Name=lx18 State=DOWN
Name=lx19 
Name=lx20 
Name=lx21 
Name=lx22 CPUs=8
Name=lx23 
Name=lx24 
Name=lx25 
Name=lx26 
Name=lx27 
Name=lx28 
Name=lx29 
Name=lx30 
#
# lx31-lx32 for partitions 4 (class) and 3 (super)
Name=DEFAULT Partition=3,4
Name=lx31 
Name=lx32 
</pre>

<a name="SamplePart"><h2>Sample Partition Configuration File</h2></a>
<pre>
# 
# Example /usr/local/SLURM/PartitionConf
# Author: John Doe
# Date: 12/14/2001
#
Name=pbatch  Number=0 JobType=BATCH       MaxCpus=128 MaxTime=UNLIMITED
Name=debug   Number=1 JobType=INTERACTIVE MaxCpus=16  MaxTime=60
Name=super   Number=3 JobType=ALL   MaxCpus=UNLIMITED MaxTime=UNLIMITED AllowUsers=cdunlap,garlick,grondo,jette
Name=class   Number=4 JobType=ALL         MaxCpus=16  MaxTime=10        AllowUsers=student1,student2,student3
</pre>

<a name="SampleAdmin"><h2>Sample slurm_admin Execution</h2></a>
<pre>
Upload actual node configurations to review:
  # slurm_admin
  slurm_admin: upload
  slurm_admin: write node baseline_node_config
  slurm_admin: reload
  slurm_admin: quit
  # cat baseline_node_config
  .....

Remove node lx30 from service, removing jobs as needed:
  # slurm_admin
  slurm_admin: update node Name=lx30 State=DRAINING
  slurm_admin: show job
  ID=1234 Name=Simulation JobType=BATCH MaxTime=100 Nodes=lx29,lx30 State=RUNNING User=grondo
  ID=1235 Name=MyBigTest  JobType=BATCH MaxTime=100 Nodes=lx20,lx21 State=RUNNING User=grondo
  slurm_admin: update job ID=1234 State=ENDING
  slurm_admin: show job 1234
  Job 1234 not found
  slurm_admin: stop lx30
  slurm_admin: show node lx30
  Name=lx30 Partition=0,3 State=DOWN OS=Linux.2.4.7-1 CPUs=16 Speed=345.0 RealMemory=2048 VirtualMemory=4096 TmpDisk=16384
  slurm_admin: quit
</pre>

<hr>
URL = http://www-lc.llnl.gov/dctg-lc/slurm/user.administrator.html
<p>Last Modified January 25, 2002</p>
<address>Maintained by Moe Jette <a href="mailto:jette@llnl.gov">
jette1@llnl.gov</a></address>
</body>
</html>
