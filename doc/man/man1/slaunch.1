.\" $Id$
.TH "slaunch" "1" "SLURM 1.2" "October 2006" "SLURM Commands"
.SH "NAME"
.LP 
slaunch \- Launch a parallel application under a SLURM job allocation.
.SH "SYNOPSIS"
.LP 
slaunch [\fIoptions\fP] <\fIcommand\fP> [\fIcommand args\fR]
.SH "DESCRIPTION"
.LP 
slaunch launches a parallel application (a \fBjob step\fR in SLURM parlance)
on the nodes, or subset of nodes, in a \fBjob allocation\fR.   A valid job
allocation is a prerequisite of running slaunch.  The ID of the job allocation
may be passed to slaunch through either the \fB\-\-jobid\fR command line
parameter or the \fBSLAUNCH_JOBID\fR environment variable.  The \fBsalloc\fR
and \fBsbatch\fR commands may be used to request a job allocation, and each
of those commands automatically set the \fBSLURM_JOB_ID\fR environment variable,
which is also understood by slaunch.  Users should not set SLURM_JOB_ID on their
own; use SLAUNCH_JOBID instead.

.SH "OPTIONS"
.LP 

.TP
\fB\-C\fR, \fB\-\-overcommit\fR
Permit the allocation of more tasks to a node than there are available processors.
Normally SLURM will only allow up to N tasks on a node with N processors, but
this option will allow more than N tasks to be assigned to a node.

.TP 
\fB\-c\fR, \fB\-\-cpus\-per\-task\fR[=]<\fIncpus\fR>
Specify that each task requires \fIncpus\fR number of CPUs.  Useful for applications in which each task will launch multiple threads and can therefore benefit from there being free processors on the node.

.TP
\fB\-\-comm\-hostname\fR[=]<\fIhostname|address\fR>
Specify the hostname or address to be used for PMI communications only
(MPCIH2 communication bootstrapping mechanism).
Defaults to short hostname of the node on which slaunch is running.

.TP
\fB\-\-core\fR[=]<\fItype\fR>
Adjust corefile format for parallel job. If possible, slaunch will set
up the environment for the job such that a corefile format other than
full core dumps is enabled. If run with type = "list", slaunch will
print a list of supported corefile format types to stdout and exit.

.TP
\fB\-\-cpu_bind\fR=[{\fIquiet,verbose\fR},]\fItype\fR
Bind tasks to CPUs. Used only when the task/affinity or task/numa 
plugin is enabled. 
NOTE: To have SLURM always report on the selected CPU binding for all 
commands executed in a shell, you can enable verbose mode by setting 
the SLURM_CPU_BIND environment variable value to "verbose". 
Supported options include:
.PD 1
.RS
.TP
.B q[uiet],
quietly bind before task runs (default)
.TP
.B v[erbose],
verbosely report binding before task runs
.TP
.B no[ne]
don't bind tasks to CPUs (default)
.TP
.B rank
bind by task rank
.TP
.B map_cpu:<list>
bind by mapping CPU IDs to tasks as specified
where <list> is <cpuid1>,<cpuid2>,...<cpuidN>.
CPU IDs are interpreted as decimal values unless they are preceded
with '0x' in which case they interpreted as hexadecimal values.
.TP
.B mask_cpu:<list>
bind by setting CPU masks on tasks as specified
where <list> is <mask1>,<mask2>,...<maskN>.
CPU masks are \fBalways\fR interpreted as hexadecimal values but can be
preceded with an optional '0x'.
.RE

.TP 
\fB\-D\fR, \fB\-\-workdir\fR[=]<\fIdirectory\fR>
Set the working directory of the tasks to \fIdirectory\fR before execution.
The default task working directory is slaunch's working directory.

.TP
\fB\-d\fR, \fB\-\-slurmd\-debug\fR[=]<\fIlevel\fR>
Specify a debug level for slurmd(8). \fIlevel\fR may be an integer value
between 0 [quiet, only errors are displayed] and 4 [verbose operation]. 
The slurmd debug information is copied onto the stderr of
the job.  By default only errors are displayed. 

.TP
\fB\-E\fR, \fB\-\-task\-error\fR[=]<\fIfilename pattern\fR>
Instruct SLURM to connect each task's standard error directly to 
the file name specified in the "\fIfilename pattern\fR".
See the \fB\-\-task\-input\fR option for filename specification options.

.TP 
\fB\-e\fR, \fB\-\-slaunch\-error\fR[=]<\fIfilename pattern\fR>
Instruct SLURM to connect slaunch's standard error directly to the 
file name specified in the "\fIfilename pattern\fR".
See the \fB\-\-slaunch\-input\fR option for filename specification options.

.TP
\fB\-\-epilog\fR[=]<\fIexecutable\fR>
\fBslaunch\fR will run \fIexecutable\fR just after the job step completes.
The command line arguments for \fIexecutable\fR will be the command
and arguments of the job step.  If \fIexecutable\fR is "none", then
no epilog will be run.  This parameter overrides the SrunEpilog
parameter in slurm.conf.

.TP 
\fB\-F\fR, \fB\-\-task\-layout\-file\fR[=]<\fIfilename\fR>
Request a specific task layout.  This option is much like the
\-\-task\-layout\-byname option, except that instead of a nodelist you
supply the name of a file.  The file contains a nodelist that may span
multiple lines of the file.

NOTE: This option implicitly sets the task distribution method to "arbitrary".
Some network switch layers do not permit arbitrary task layout.

.TP
\fB\-\-gid\fR[=]<\fIgroup\fR>
If \fBslaunch\fR is run as root, and the \fB\-\-gid\fR option is used, 
submit the job with \fIgroup\fR's group access permissions.  \fIgroup\fR 
may be the group name or the numerical group ID.

.TP 
\fB\-h\fR, \fB\-\-help\fR
Display help information and exit.

.TP
\fB\-I\fR, \fB\-\-task\-input\fR[=]<\fIfilename pattern\fR>
Instruct SLURM to connect each task's standard input directly to 
the file name specified in the "\fIfilename pattern\fR".

By default, the standard IO streams of all tasks are received and transmitted
over the network to commands like slaunch and sattach.  These options disable
the networked standard IO streams and instead connect the standard IO streams
of the tasks directly to files on the local node of each task (although the file
may, of course, be located on a networked filesystem).

Whether or not the tasks share a file depends on whether or not the file lives
on a local filesystem or a shared network filesytem, and on whether or not
the filename pattern expands to the same file name for each task.

The filename pattern may
contain one or more replacement symbols, which are a percent sign "%" followed 
by a letter (e.g. %t).

Supported replacement symbols are:
.PD 0
.RS 10
.TP 
\fB%J\fR
Job allocation number and job step number in the form "jobid.stepid".  For instance, "128.0".
.PD 0
.TP 
\fB%j\fR
Job allocation number.
.PD 0
.TP 
\fB%s\fR
Job step number.
.PD 0
.TP 
\fB%N\fR
Node name. (Will result in a separate file per node.)
.PD 0
.TP 
\fB%n\fR
Relative node index number within the job step.  All nodes used by the job step will be number sequentially starting at zero.  (Will result in a separate file per node.)
.PD 0
.TP 
\fB%t\fR
Task rank number.  (Will result in a separate file per task.)
.RE

.TP 
\fB\-i\fR, \fB\-\-slaunch\-input\fR[=]<\fIfilename pattern\fR>
.PD
Change slaunch's standard input
to be a file of name "filename pattern".  These options are similar to using
shell IO redirection capabilities, but with the additional ability to replace
certain symbols in the filename with useful SLURM information.  Symbols are
listed below.

By default, slaunch broadcasts its standard input over the network to the
standard input of all tasks.  Likewise, standard output and standard error
from all tasks are collected over the network by slaunch and printed on
its standard output or standard error, respectively.  If you want to see
traffic from fewer tasks, see the \-\-slaunch\-[input|output|error]\-filter
options.

Supported replacement symbols are:
.PD 0
.RS 10
.TP 
\fB%J\fR
Job allocation number and job step number in the form "jobid.stepid".  For instance, "128.0".
.PD 0
.TP 
\fB%j\fR
Job allocation number.
.PD 0
.TP 
\fB%s\fR
Job step number.
.RE

.TP
\fB\-J\fR, \fB\-\-name\fR[=]<\fIname\fR>
Set the name of the job step.  By default, the job step's name will be the
name of the executable which slaunch is launching.

.TP 
\fB\-\-jobid\fR=<\fIJOBID\fP>
The job allocation under which the parallel application should be launched.  If slaunch is running under salloc or a batch script, slaunch can automatically determint the jobid from the SLURM_JOB_ID environment variable.  Otherwise, you will need to tell slaunch which job allocation to use.

.TP
\fB\-K\fR, \fB\-\-kill\-on\-bad\-exit\fR
Terminate the job step if any task exits with a non\-zero exit code.  By default
slaunch will not terminate a job step because of a task with a non\-zero exit
code.

.TP 
\fB\-L\fR, \fB\-\-nodelist\-byid\fR[=]<\fInode index list\fR>
Request a specific set of nodes in a job alloction on which to run the tasks of the job step.  The list may be specified as a comma\-separated list relative node indices in the job allocation (e.g., "0,2\-5,\-2,8").  Duplicate indices are permitted, but are ignored.  The order of the node indices in the list is not important; the node indices will be sorted my SLURM.

.TP
\fB\-l\fR, \fB\-\-label\fR
Prepend each line of task standard output or standard error with the task
number of its origin.

.TP
\fB\-m\fR, \fB\-\-distribution\fR=
(\fIblock\fR|\fIcyclic\fR|\fIhostfile\fR|\fIplane=<options>\fR)
Specify an alternate distribution method for remote processes.
.PD 1
.RS
.TP
.B block
The block method of distribution will allocate processes in\-order to
the cpus on a node. If the number of processes exceeds the number of
cpus on all of the nodes in the allocation then all nodes will be
utilized. For example, consider an allocation of three nodes each with
two cpus. A four\-process block distribution request will distribute
those processes to the nodes with processes one and two on the first
node, process three on the second node, and process four on the third node.
Block distribution is the default behavior if the number of tasks
exceeds the number of nodes requested.
.TP
.B cyclic
The cyclic method distributes processes in a round\-robin fashion across
the allocated nodes. That is, process one will be allocated to the first
node, process two to the second, and so on. This is the default behavior
if the number of tasks is no larger than the number of nodes requested.
.TP
.B plane
The tasks are distributed in blocks of a specified size.
The options include a number representing the size of the task block.
This is followed by an optional specification of the task distribution
scheme within a block of tasks and between the blocks of tasks.
For more details (including examples and diagrams), please see
http://www.llnl.gov/linux/slurm/mc_support.html and
http://www.llnl.gov/linux/slurm/dist_plane.html.
.TP
.B hostfile
The hostfile method of distribution will allocate processes in\-order as
listed in file designated by the environment variable SLURM_HOSTFILE.  If
this variable is listed it will over ride any other method specified.
.RE

.TP
\fB\-\-mem_bind\fR=[{\fIquiet,verbose\fR},]\fItype\fR
Bind tasks to memory. 
Used only when task/affinity plugin is enabled and 
the NUMA memory functions are available
\fBNote that the resolution of CPU and memory binding 
may differ on some architectures.\fR For example, CPU binding may be performed 
at the level of the cores within a processor while memory binding will 
be performed at the level of nodes, where the definition of "nodes" 
may differ from system to system. \fBThe use of any type other than 
"none" or "local" is not recommended.\fR
If you want greater control, try running a simple test code with the 
options "\-\-cpu_bind=verbose,none \-\-mem_bind=verbose,none" to determine 
the specific configuration.
Note: To have SLURM always report on the selected memory binding for all 
commands executed in a shell, you can enable verbose mode by setting the 
SLURM_MEM_BIND environment variable value to "verbose".
Supported options include:
.PD 1
.RS
.TP
.B q[uiet],
quietly bind before task runs (default)
.TP
.B v[erbose],
verbosely report binding before task runs
.TP
.B no[ne]
don't bind tasks to memory (default)
.TP
.B rank
bind by task rank (not recommended)
.TP
.B local
Use memory local to the processor in use
.TP
.B map_mem:<list>
bind by mapping a node's memory to tasks as specified
where <list> is <cpuid1>,<cpuid2>,...<cpuidN>.
CPU IDs are interpreted as decimal values unless they are preceded
with '0x' in which case they interpreted as hexadecimal values
(not recommended)
.TP
.B mask_mem:<list>
bind by setting memory masks on tasks as specified
where <list> is <mask1>,<mask2>,...<maskN>.
memory masks are \fBalways\fR interpreted as hexadecimal values but can be
preceded with an optional '0x' (not recommended)
.RE

.TP 
\fB\-\-mpi\fR[=]<\fImpi_type\fR>
Identify the type of MPI to be used. If run with mpi_type = "list", 
slaunch will print a list of supported MPI types to stdout and exit.

.TP
\fB\-\-multi\-prog\fR
This option allows one to launch tasks with different executables within
the same job step.  When this option is present, slaunch no long accepts
the name of an executable "command" on the command line, instead it accepts
the name of a file.  This file specifies which executables and command line
parameters should be used by each task in the job step.  See the section 
\fBMULTIPLE PROGRAMS FILE\fR below for an explanation of the multiple program
file syntax.

.TP 
\fB\-N\fR, \fB\-\-nodes\fR[=]<\fInumber\fR>
Specify the number of nodes to be used by this job step.  By default,
slaunch will use all of the nodes in the specified job allocation.

.TP 
\fB\-n\fR, \fB\-\-tasks\fR[=]<\fInumber\fR>
Specify the number of processes to launch.  The default is one process per node.

.TP
\fB\-\-network\fR[=]<\fIoptions\fR>
(NOTE: this option is currently only of use on AIX systems.)
Specify the communication protocol to be used. 
The interpretation of \fItype\fR is system dependent. 
For AIX systems with an IBM Federation switch, the following 
comma\-separated and case insensitive options are recongnized:
\fBIP\fR (the default is user\-space), \fBSN_ALL\fR, \fBSN_SINGLE\fR, 
\fBBULK_XFER\fR and adapter names. For more information, on 
IBM systems see \fIpoe\fR documenation on the environment variables 
\fBMP_EUIDEVICE\fR and \fBMP_USE_BULK_XFER\fR.

.TP
\fB\-O\fR, \fB\-\-task\-output\fR[=]<\fIfilename pattern\fR>
Instruct SLURM to connect each task's standard output directly to 
the file name specified in the "\fIfilename pattern\fR".
See the \fB\-\-task\-input\fR option for filename specification options.

.TP
\fB\-o\fR, \fB\-\-slaunch\-output\fR[=]<\fIfilename pattern\fR>
Instruct SLURM to connect slaunch's standard output directly to the 
file name specified in the "\fIfilename pattern\fR".
See the \fB\-\-slaunch\-input\fR option for filename specification options.

.TP
\fB\-\-propagate\fR[=\fIrlimits\fR]
Allows users to specify which of the modifiable (soft) resource limits
to propagate to the compute nodes and apply to their jobs.  If
\fIrlimits\fR is not specified, then all resource limits will be
propagated.
The following rlimit names are supported by Slurm (although some
options may not be supported on some systems):
.RS
.TP 10
\fBAS\fR
The maximum address space for a processes
.TP
\fBCORE\fR
The maximum size of core file
.TP
\fBCPU\fR
The maximum amount of CPU time
.TP
\fBDATA\fR
The maximum size of a process's data segment
.TP
\fBFSIZE\fR
The maximum size of files created
.TP
\fBMEMLOCK\fR
The maximum size that may be locked into memory
.TP
\fBNOFILE\fR
The maximum number of open files
.TP
\fBNPROC\fR
The maximum number of processes available
.TP
\fBRSS\fR
The maximum resident set size
.TP
\fBSTACK\fR
The maximum stack size
.RE

.TP
\fB\-\-prolog\fR[=]<\fIexecutable\fR>
\fBslaunch\fR will run \fIexecutable\fR just before launching the job step.
The command line arguments for \fIexecutable\fR will be the command
and arguments of the job step.  If \fIexecutable\fR is "none", then
no prolog will be run.  This parameter overrides the SrunProlog
parameter in slurm.conf.

.TP
\fB\-q\fR, \fB\-\-quiet\fR
Suppress informational messages from slaunch. Errors will still be displayed.

.TP 
\fB\-r\fR, \fB\-\-relative\fR[=]<\fInumber\fR>
Specify the first node in the allocation on which this job step will be launched.  Counting starts at zero, thus the first node in the job allocation is node 0.  The option to \-\-relative may also be a negative number.  \-1 is the last node in the allocation, \-2 is the next to last node, etc.  By default, the controller will select the starting node (assuming that there are no other nodelist or task layout options that specify specific nodes).

.TP 
\fB\-\-slaunch\-input\-filter\fR[=]<\fItask number\fR>
.PD 0
.TP
\fB\-\-slaunch\-output\-filter\fR[=]<\fItask number\fR>
.PD 0
.TP 
\fB\-\-slaunch\-error\-filter\fR[=]<\fItask number\fR>
.PD
Only transmit standard input to a single task, or print the standard output
or standard error from a single task.  These options perform the filtering
locally in slaunch.  All tasks are still capable of sending or receiving
standard IO over the network, so the "sattach" command can still access the
standard IO streams of the other tasks.  (NOTE: for \-output and \-error,
the streams from all tasks WILL be transmitted to slaunch, but it will only
print the streams for the selected task.  If your tasks print a great deal of
data to standard output or error, this can be performance limiting.)

.TP 
\fB\-T\fR, \fB\-\-task\-layout\-byid\fR[=]<\fInode index list\fR>
Request a specific task layout using node indices within the job allocation.  The node index list can contain duplicate indices, and the indices may appear in any order.  The order of indices in the nodelist IS significant.  Each node index in the list represents one task, with the Nth node index in the list designating on which node the Nth task should be launched.

For example, given an allocation of nodes "linux[0\-15]" and a node index list "4,\-1,1\-3" task 0 will run on "linux4", task 1 will run on "linux15", task 2 on "linux1", task 3 on "linux2", and task 4 on "linux3".

NOTE: This option implicitly sets the task distribution method to "arbitrary".  Some network switch layers do not permit arbitrary task layout.

.TP
\fB\-\-task\-epilog\fR[=]<\fIexecutable\fR>
The \fBslurmd\fR daemon will run \fIexecutable\fR just after each task
terminates. This will be before after any TaskEpilog parameter      
in slurm.conf is executed. This is meant to be a very short\-lived 
program. If it fails to terminate within a few seconds, it will 
be killed along with any descendant processes.

.TP
\fB\-\-task\-prolog\fR[=]<\fIexecutable\fR>
The \fBslurmd\fR daemon will run \fIexecutable\fR just before launching 
each task. This will be executed after any TaskProlog parameter 
in slurm.conf is executed.
Besides the normal environment variables, this has SLURM_TASK_PID
available to identify the process ID of the task being started.
Standard output from this program of the form
"export NAME=value" will be used to set environment variables
for the task being spawned.

.TP
\fB\-u\fR, \fB\-\-unbuffered\fR
Do not line buffer standard output or standard error from remote tasks.
This option cannot be used with \-\-label.

.TP
\fB\-\-uid\fR[=]<\fIuser\fR>
Attempt to submit and/or run a job as \fIuser\fR instead of the
invoking user id. The invoking user's credentials will be used
to check access permissions for the target partition. User root
may use this option to run jobs as a normal user in a RootOnly
partition for example. If run as root, \fBslaunch\fR will drop
its permissions to the uid specified after node allocation is
successful. \fIuser\fR may be the user name or numerical user ID.

.TP
\fB\-\-usage\fR
Display brief usage message and exit.

.TP 
\fB\-V\fR, \fB\-\-version\fR
Display version information and exit.

.TP
\fB\-v\fR, \fB\-\-verbose\fR
Increase the verbosity of slaunch's informational messages.  Multiple \-v's
will further increase slaunch's verbosity.

.TP
\fB\-W\fR, \fB\-\-wait\fR[=]<\fIseconds\fR>
slaunch will wait the specified number of seconds after the first tasks exits
before killing all tasks in the job step.  If the value is 0, slaunch will
wait indefinitely for all tasks to exit.  The default value is give by the
WaitTime parameter in the slurm configuration file (see \fBslurm.conf(5)\fR).

The \-\-wait option can be used to insure that a job step terminates in a timely
fashion in the event that one or more tasks terminate prematurely.

.TP 
\fB\-w\fR, \fB\-\-nodelist\-byname\fR[=]<\fInode name list\fR>
Request a specific list of node names.  The list may be specified as a comma\-separated list of node names, or a range of node names (e.g. mynode[1\-5,7,...]).  Duplicate node names are not permitted in the list.
The order of the node names in the list is not important; the node names
will be sorted my SLURM.

.TP 
\fB\-Y\fR, \fB\-\-task\-layout\-byname\fR[=]<\fInode name list\fR>
Request a specific task layout.  The nodelist can contain duplicate node
names, and node names may appear in any order.  The order of node names in
the nodelist IS significant.  Each node name in the nodes list represents
one task, with the Nth node name in the nodelist designating on which node
the Nth task should be launched.  For example, a nodelist of mynode[4,3,1\-2,4]
means that tasks 0 and 4 will run on mynode4, task 1 will run on mynode3,
task 2 will run on mynode1, and task 3 will run on mynode2.

NOTE: This option implicitly sets the task distribution method to "arbitrary".
Some network switch layers do not permit arbitrary task layout.
 
.SH "INPUT ENVIRONMENT VARIABLES"
.PP
Some slaunch options may be set via environment variables. 
These environment variables, along with their corresponding options, 
are listed below.
Note: Command line options will always override environment variables settings.
.TP 25
\fBSLAUNCH_COMM_HOSTNAME\fR
Same as \fB\-\-comm\-hostname\fR.
.TP
\fBSLAUNCH_CORE_FORMAT\fR
Same as \fB\-\-core\fR.
.TP
\fBSLAUNCH_CPU_BIND\fR
Same as \fB\-\-cpu_bind\fR.
.TP
\fBSLAUNCH_DEBUG\fR
Same as \fB\-v\fR or \fB\-\-verbose\fR.
.TP
\fBSLAUNCH_DISTRIBUTION\fR
Same as \fB\-m\fR or \fB\-\-distribution\fR.
.TP
\fBSLAUNCH_JOBID\fR
Same as \fB\-\-jobid\fR.
.TP
\fBSLAUNCH_KILL_BAD_EXIT\fR
Same as \fB\-K\fR or \fB\-\-kill\-on\-bad\-exit\fR.
.TP
\fBSLAUNCH_LABELIO\fR
Same as \fB\-l\fR or \fB\-\-label\fR.
.TP
\fBSLAUNCH_MEM_BIND\fR
Same as \fB\-\-mem_bind\fR.
.TP
\fBSLAUNCH_MPI_TYPE\fR
Same as \fB\-\-mpi\fR.
.TP
\fBSLAUNCH_OVERCOMMIT\fR
Same as \fB\-C\fR or \fB\-\-overcomit\fR.
.TP
\fBSLAUNCH_WAIT\fR
Same as \fB\-W\fR or \fB\-\-wait\fR.
.TP
\fBSLURMD_DEBUG\fR
Same as \fB\-d\fR or \fB\-\-slurmd\-debug\fR

.SH "OUTPUT ENVIRONMENT VARIABLES"
.PP
slaunch will set the following environment variables which will
appear in the environments of all tasks in the job step.  Since slaunch
sets these variables itself, they will also be available to --prolog
and --epilog scripts.  (Notice that the "backwards compatibility" environment
variables clobber some of the variables that were set by salloc or sbatch
at job allocation time.  The newer SLURM_JOB_* and SLURM_STEP_* names do not
conflict, so any task in any job step can easily determine the parameters
of the job allocation.)
.TP
\fBSLURM_STEP_ID\fR (and \fBSLURM_STEPID\fR for backwards compatibility)
The ID of the job step within the job allocation.
.TP
\fBSLURM_STEP_NODELIST\fR
The list of nodes in the job step.
.TP
\fBSLURM_STEP_NUM_NODES\fR (and \fBSLURM_NNODES\fR for backwards compatibility)
The number of nodes used by the job step.
.TP
\fBSLURM_STEP_NUM_TASKS\fR (and \fBSLURM_NPROCS\fR for backwards compatibility)
The number of tasks in the job step.
.TP
\fBSLURM_STEP_TASKS_PER_NODE\fR (and \fBSLURM_TASKS_PER_NODE\fR for backwards compatibility)
The number of tasks on each node in the job step.
.TP
\fBSLURM_STEP_LAUNCHER_HOSTNAME\fR (and \fBSLURM_SRUN_COMM_HOST\fR for backwards compatibility)
.TP
\fBSLURM_STEP_LAUNCHER_PORT\fR (and \fBSLURM_SRUN_COMM_PORT\fR for backwards compatibility)

.PP
Additionally, SLURM daemons will ensure that the the following variables are
set in the environments of all tasks in the job step.  Many of the following
variables will have different values in each task's environment.  (These
variables are not available to the slaunch --prolog and --epilog scripts.)

.TP
\fBSLURM_NODEID\fR
Node ID relative to other nodes in the job step.  Counting begins at zero.
.TP
\fBSLURM_PROCID\fR
Task ID relative to the other tasks in the job step.  Counting begins at zero.
.TP
\fBSLURM_LOCALID\fR
Task ID relative to the other tasks on the same node which belong to the
same job step.  Counting begins at zero.
.TP
\fBSLURMD_NODENAME\fR
The SLURM NodeName for the node on which the task is running.  Depending
on how your system administrator has configured SLURM, the NodeName for a
node may not be the same as the node's hostname.  When you use commands
such as \fBsinfo\fR and \fBsqueue\fR, or look at environment variables such
as SLURM_JOB_NODELIST and SLURM_STEP_NODELIST, you are seeing SLURM NodeNames.

.SH "MULTIPLE PROGRAMS FILE"
Comments in the configuration file must have a "#" in collumn one.
The configuration file contains the following fields separated by white
space:
.TP
Task rank
One or more task ranks to use this configuration.
Multiple values may be comma separated.
Ranges may be indicated with two numbers separated with a '\-'.
To indicate all tasks, specify a rank of '*' (in which case you probably 
should not be using this option).
.TP
Executable
The name of the program to execute.
May be fully qualified pathname if desired.
.TP
Arguments
Program arguments.
The expression "%t" will be replaced with the task's number.
The expression "%o" will be replaced with the task's offset within
this range (e.g. a configured task rank value of "1\-5" would
have offset values of "0\-4").
Single quotes may be used to avoid having the enclosed values interpretted.
This field is optional.
.PP
For example:
.nf
###################################################################
# srun multiple program configuration file
#
# srun \-n8 \-l \-\-multi\-prog silly.conf
###################################################################
4\-6       hostname
1,7       echo  task:%t
0,2\-3     echo  offset:%o

$ srun \-n8 \-l \-\-multi\-prog silly.conf
0: offset:0
1: task:1
2: offset:1
3: offset:2
4: linux15.llnl.gov
5: linux16.llnl.gov
6: linux17.llnl.gov
7: task:7

.fi

.SH "EXAMPLES"
.LP 
To launch a job step (parallel program) in an existing job allocation:
.IP 
slaunch \-\-jobid 66777 \-N2 \-n8 myprogram

.LP 
To grab an allocation of nodes and launch a parallel application on one command line (See the \fBsalloc\fR man page for more examples):
.IP 
salloc \-N5 slaunch \-n10 myprogram

.SH "COPYING"
Copyright (C) 2006\-2007 The Regents of the University of California.
Produced at Lawrence Livermore National Laboratory (cf, DISCLAIMER).
UCRL\-CODE\-226842.
.LP
This file is part of SLURM, a resource management program.
For details, see <http://www.llnl.gov/linux/slurm/>.
.LP
SLURM is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation; either version 2 of the License, or (at your option)
any later version.
.LP
SLURM is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

.SH "SEE ALSO"
.LP 
sinfo(1), sattach(1), salloc(1), sbatch(1), squeue(1), scancel(1), scontrol(1), slurm.conf(5), sched_setaffinity(2), numa(3)
