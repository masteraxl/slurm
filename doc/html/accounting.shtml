<!--#include virtual="header.txt"-->

<h1>Accounting</h1>
<p>SLURM collects accounting information for every job and job step 
executed. 
This information can be viewed using the <b>sacct</b> command. 
Information is available about both currently executing jobs and 
jobs which have already terminated. 
Resource usage is reported for each task and this can be useful to 
detect load imbalance between the tasks. 
SLURM version 1.2 and earlier supported the storage of accounting 
records to a text file.
Beginning in SLURM version 1.3 accounting records can be written to 
a database. </p>

<p>There are three disticnt plugins associated with resource accounting.
The configuration parameters associated with these plugins include:
<ul>
<li><b>JobCompType</b> controls how job completion information is 
recorded. This can be used to record basic job information such
as job name, user name, allocated nodes, start time, completion 
time, exit status, etc. If the preservation of only basic job 
information is required, this plugin should satisfy your needs
with minimal overhead. You can store this information in a 
text file, <a href="http://www.mysql.com/">MySQL</a> or 
<a href="http://www.postgresql.org/">PostgreSQL</a> 
database and use either 
<a href="http://www.clusterresources.com/pages/products/gold-allocation-manager.php">Gold</a>
or SlurmDBD for added security.</li>
<li><b>JobAcctGatherType</b> is operating system dependent and 
controls what mechanisms are used to collect accounting information.
Supported values are <i>jobacct_gather/aix</i>, <i>jobacct_gather/linux</i>
and <i>jobacct_gather/none</i> (no information collected).</li>
<li><b>AccountingStorageType</b> controls how detailed job and job 
step information is recorded. You can store this information in a 
text file, <a href="http://www.mysql.com/">MySQL</a> or 
<a href="http://www.postgresql.org/">PostgreSQL</a> 
database and use either 
<a href="http://www.clusterresources.com/pages/products/gold-allocation-manager.php">Gold</a>
or SlurmDBD for added security.</li>
</ul>

<p>Storing the information into text files is very simple. 
Just configure the appropriate plugin (e.g. 
<i>AccountingStorageType=accounting_storage/filetxt</i> and/or 
<i>JobCompType=jobcomp/filetxt</i>) and then specify the 
pathname of the file (e.g.
<i>AccountingStorageLoc=/var/log/slurm/accounting</i> and/or 
<i>JobCompLoc=/var/log/slurm/job_completions</i>).
Use the <i>logrotate</i> or similar tool to prevent the 
log files from getting too large.
Send a SIGHUP signal to the <i>slurmctld</i> deaemon 
after moving the files, but before compressing them so
that new log files will be created.</p>

<p>Storing the data directly into a database from SLURM may seem 
attractive, but that requires the availability of user name and 
password data not only for the SLURM control daemon (slurmctld), 
but also user commands which need to access the data (sacct and
sacctmgr). 
Making information available to all users makes database security 
more difficult to provide, sending the data through an intermediate
daemon can provide better security. 
<a href="http://www.clusterresources.com/pages/products/gold-allocation-manager.php">Gold</a>
and SlurmDBD are two such services. 
Our initial implementation relied upon Gold, but we found its
performance to be inadequate for our needs and developed SlurmDBD.
SlurmDBD (SLURM DataBase Deaemon) is written in C, multi-threaded, 
secure, and orders of magnitude faster than Gold.
The configuration required to use SlurmDBD will be described below.
Direct database or Gold use would be similar.</p>

<h2>Infrastructure</h2>

<p>If the SlurmDBD is executed on a different cluster than the 
one managed by SLURM, possibly to collect data from multiple 
clusters in a single location, then a uniform user ID
space must be provided across all computers (the users need not 
be permitted to login to every machine, but every user with an 
account on one of the SLURM managed clusters must have an account
on the machine where SlurmDBD executes and that user must have 
the same user name and user ID).</p>

<p>The best way to insure security of the data is by authenticating 
communications to the SlurmDBD and we recommend 
<a href="http://home.gna.org/munge/">Munge</a> for that purpose.
Munge was designed to support authentication within a cluster.
If you have one cluster managed by SLURM and execute the SlurmDBD 
on that one cluster, the normal Munge configuration will suffice.
Otherwise Munge should then be installed on all nodes of all 
SLURM managed clusters plus the machine where SlurmDBD executes.
You then have a choice of either having a single Munge key for 
all of these computers or maintaining a unique key for each of the 
clusters plus a second key for communications between the clusters
for better security.
Munge enhancements are planned to support two keys within a single 
configuration file, but presently two different daemons must be 
started with different configuration files to support two different 
keys. 
If a Munge separate daemon configured to provide enterprise-wide 
authentication, it will have a unique named pipe configured for 
communications. 
The pathname of this pipe will be needed in the SLURM and SlurmDBD
configuration files (slurm.conf and slurmdbd.conf respectively, 
more details are provided below).</p>

<h2>SLURM Configuration</h2>

<p>Several SLURM configuration parameters must be set to support
archiving information in SlurmDBD. SlurmDBD has a separate configuration
file which is documented in a separate section.
Note that you can write accounting information to SlurmDBD
while job completion records are written to a text file or 
not maintained at all. 
If you don't set the configuration parameters that begin 
with "JobComp" then job completion records will not be recorded.</p>

<ul>
<li><b>AccountingStorageEnforce</b>:
If you want to prevent users from running jobs if their <i>association</i>
(a combintation of cluster name, partition name, user name, and account name)
is not in the database, then set this to "1".
Otherwise jobs will be executed based upon policies configured in 
SLURM on each cluster.</li>

<li><b>AccountingStorageHost</b>:
The name or address of the host where SlurmDBD executes.</li>

<li><b>AccountingStoragePass</b>:
If using SlurmDBD with a second Munge daemon, store the pathname of 
the named socket used by Munge to provide enterprise-wide.
Otherwise the default Munge daemon will be used.</li>

<li><b>AccountingStoragePort</b>:
The network port that SlurmDBD accepts communication on.</li>

<li><b>AccountingStorageType</b>:
Set to "accounting_storage/slurmdbd".</li>

<li><b>ClusterName</b>:
Set to a unique name for each Slurm-managed cluster so that 
accounting records from each can be identified.</li>

<li><b>JobCompHost</b>:
The name or address of the host where SlurmDBD executes.</li>

<li><b>JobCompPass</b>:
If using SlurmDBD with a second Munge daemon, store the pathname of 
the named socket used by Munge to provide enterprise-wide.
Otherwise the default Munge daemon will be used.</li>

<li><b>JobCompPort</b>:
The network port that SlurmDBD accepts communication on.</li>

<li><b>JobCompType</b>:
Set to "jobcomp/slurmdbd".</li>
</ul>

<h2>SlurmDBD Configuration</h2>

<p>SlurmDBD requires its own configuration file called "slurmdbd.conf". 
This file should be only on the computer where SlurmDBD executes and 
should only be readable by the user which executes SlurmDBD (e.g. "slurm").
The configuration parameters required by SlurmDBD include:</p>

<ul>
<li><b>TBD</b>:
TBD.</li>
</ul>

<h2>Database Configuration</h2>

<p>Accounting records are maintained based upon what we refer 
to as an <i>Association</i>, which consists of four elements:
cluster name, partition name, user name, and account name. </p>

<p>TBD</p>

<p style="text-align:center;">Last modified 12 March 2008</p>

<!--#include virtual="footer.txt"-->
