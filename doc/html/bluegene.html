<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                        "http://www.w3.org/TR/REC-html40/loose.dtd">

<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="keywords" content="Simple Linux Utility for Resource Management, SLURM, resource management, 
Linux clusters, high-performance computing, Livermore Computing">
<meta name="LLNLRandR" content="UCRL-WEB-213976">
<meta name="LLNLRandRdate" content="15 October 2005">
<meta name="distribution" content="global">
<meta name="description" content="Simple Linux Utility for Resource Management">
<meta name="copyright"
content="This document is copyrighted U.S.
Department of Energy under Contract W-7405-Eng-48">
<meta name="Author" content="Morris Jette">
<meta name="email" content="jette1@llnl.gov">
<meta name="Classification"
content="DOE:DOE Web sites via organizational
structure:Laboratories and Other Field Facilities">
<title>Simple Linux Utility for Resource Management:Blue Gene User and Administrator Guide</title>
<link href="slurmstyles.css" rel="stylesheet" type="text/css">
</head>

<body bgcolor="#000000" text="#000000" leftmargin="0" topmargin="0">
<table width="770" border="0" cellspacing="0" cellpadding="0">
<tr> 
<td><img src="slurm_banner.jpg" width="770" height="145" usemap="#Map" border="0" alt="Simple Linux Utility for Resource Management"></td>
</tr>
</table>
<table width="770" border="0" cellspacing="0" cellpadding="3" bgcolor="#FFFFFF">
<tr> 
<td width="100%"> 
<table width="760" border="0" cellspacing="0" cellpadding="4" align="right">
<tr>
<td valign="top" bgcolor="#000000"><p><img src="spacer.gif" width="110" height="1" alt=""></p>
<p><a href="slurm.html" class="nav" align="center">Home</a></p>
<p><span class="whitetext">About</span><br>
<a href="overview.html" class="nav">Overview</a><br>
<a href="news.html" class="nav">What's New</a><br>
<a href="publications.html" class="nav">Publications</a><br>
<a href="team.html" class="nav">SLURM Team</a></p>
<p><span class="whitetext">Using</span><br>
<a href="documentation.html" class="nav">Documentation</a><br>
<a href="faq.html" class="nav">FAQ</a><br>
<a href="help.html" class="nav">Getting Help</a><br>
<a href="mail.html"  class="nav">Mailing Lists</a></p>
<p><span class="whitetext">Installing</span><br>
<a href="platforms.html" class="nav">Platforms</a><br>
<a href="download.html" class="nav">Download</a><br>
<a href="quickstart_admin.html" class="nav">Guide</a></p></td>
<td><img src="spacer.gif" width="10" height="1" alt=""></td>
<td valign="top"><h2>Blue Gene User and Administrator Guide</h2>

<h3>Overview</h3>

<p>This document describes the unique features of SLURM on the
<a href="http://www.research.ibm.com/bluegene">IBM Blue Gene</a> systems. 
You should be familiar with the SLURM's mode of operation on Linux clusters 
before studying the relatively few differences in Blue Gene operation 
described in this document.</p>

<p>Blue Gene systems have several unique features making for a few 
differences in how SLURM operates there. 
The basic unit of resource allocation is a <i>base partition</i> or <i>midplane</i>.
The <i>base partitions</i> are connected in a three-dimensional torus. 
Each <i>base partition</i> includes 512 <i>c-nodes</i> each containing two processors; 
one designed primarily for computations and the other primarily for managing communications. 
SLURM considers each <i>base partition</i> as one node with 1024 processors. 
The <i>c-nodes</i> can execute only one process and thus are unable to execute both 
the user's jobs and SLURM's <i>slurmd</i> daemon.
Thus the <i>slurmd</i> daemon executes on one of the Blue Gene <i>Front End Nodes</i>. 
This single <i>slurmd</i> daemon provides (almost) all of the normal SLURM services 
for every <i>base partition</i> on the system. </p>

<h3>User Tools</h3>

<p>The normal set of SLURM user tools: srun, scancel, sinfo, squeue and scontrol 
provide all of the expected services except support for job steps. 
SLURM performs resource allocation for the job, but initiation of tasks is performed 
using the <i>mpirun</i> command. SLURM has no concept of a job step on Blue Gene.
Four new srun options are available: 
<i>--geometry</i> (specify job size in each dimension),
<i>--no-rotate</i> (disable rotation of geometry), 
<i>--conn-type</i> (specify interconnect type between base partitions, mesh or torus), and 
You can also continue to use the <i>--nodes</i> option with a minimum and (optionally) 
maximum node count. 
The <i>--ntasks</i> option continues to be supported and may be required to 
control resource allocations less than a full <i>base partition</i> in size, 
if the system is configured to support them.  
For example "srun -nntasks 256 ..." indicates that the job requires a minimum 
of 256 processors or one quarter of a midplane to execute. 
See the srun man pages for details. </p>

<p>To reiterate: srun is used to submit a job script, but mpirun is used to launch the parallel tasks. 
<b>It is highly recommended that the srun <i>--batch</i> option be used to submit a script.</b> 
Note that a SLURM batch job's default stdout and stderr file names are generated 
using the SLURM job ID. 
When the SLURM control daemon is restarted, SLURM job ID values can be repeated, 
therefore it is recommended that batch jobs explicitly specify unique names for 
stdout and stderr files using the srun options <i>--output</i> and <i>--error</i>
respectively.
While the srun <i>--allocate</i> option may be used to create an interactive SLURM job, 
it will be the responsibility of the user to insure that the <i>bgblock</i> 
is ready for use before initiating any mpirun commands. 
SLURM will assume this responsibility for batch jobs. 
The script that you submit to SLURM can contain multiple invocations of mpirun as
well as any desired commands for pre- and post-processing.
The mpirun command will get its <i>bgblock</i> or BG block information from the
<i>MPIRUN_PARTITION</i> as set by SLURM. A sample script is shown below.
<pre>
#!/bin/bash
# pre-processing
date
# processing
mpirun -exec /home/user/prog -cwd /home/user -args 123
mpirun -exec /home/user/prog -cwd /home/user -args 124
# post-processing
date 
</pre></p>
 
<a name="naming">
<p>The naming of nodes includes a three-digit suffix representing the base partition's 
location in the X, Y and Z dimensions with a zero origin.
For example, "bg012" represents the base partition whose location is at X=0, Y=1 and Z=2. 
Since jobs must be allocated consecutive nodes in all three dimensions, we have developed 
an abbreviated format for describing the nodes in one of these three-dimensional blocks. 
The node's prefix of "bg" is followed by the end-points of the block enclosed in square-brackets. 
For example, " bg[620x731]" is used to represent the eight nodes enclosed in a block 
with endpoints bg620 and bg731 (bg620, bg621, bg630, bg631, bg720, bg721, 
bg730 and bg731).</p></a>

<p>One new tool provided is <i>smap</i>.
Smap is aware of system topography and provides a map of what nodes are allocated 
to jobs, partitions, etc. 
See the smap man page for details.
A sample of smap output is provided below showing the location of five jobs. 
Note the format of the list of nodes allocated to each job.
Also note that idle (unassigned) base partitions are indicated by a period.
Down and drained base partitions (those not available for use) are 
indicated by a number sign (bg703 in the display below).
The legend is for illustrative purposes only. 
The origin (zero in every dimension) is shown at the rear left corner of the bottom plane.
Each set of four consecutive lines represents a plane in the Y dimension.
Values in the X dimension increase to the right.
Values in the Z dimension increase down and toward the left.</p>

<pre>
   a a a a b b d d       ID JOBID PARTITION BGL_BLOCK USER   NAME ST TIME NODES NODELIST
  a a a a b b d d        a  12345 batch     RMP0      joseph tst1 R  43:12   64 bg[000x333]
 a a a a b b c c         b  12346 debug     RMP1      chris  sim3 R  12:34   16 bg[420x533]
a a a a b b c c          c  12350 debug     RMP2      danny  job3 R   0:12    8 bg[622x733]
                         d  12356 debug     RMP3      dan    colu R  18:05   16 bg[600x731]
   a a a a b b d d       e  12378 debug     RMP4      joseph asx4 R   0:34    4 bg[612x713]
  a a a a b b d d
 a a a a b b c c
a a a a b b c c

   a a a a . . d d
  a a a a . . d d
 a a a a . . e e              Y
a a a a . . e e               |
                              |
   a a a a . . d d            0----X
  a a a a . . d d            /
 a a a a . . . .            /
a a a a . . . #            Z
</pre>

<p>Note that jobs enter the SLURM state RUNNING as soon as the have been 
allocated a bgblock. 
If the bgblock is in a READY state, the job will begin execution almost 
immediately. 
Otherwise the execution of the job will not actually begin until the 
bgblock is in a READY state, which can require booting the block and 
a delay of minutes to do so.
You can identify the bgblock associated with your job using the command
<i>smap -Dj -c</i> and the state of the bgblock with the command 
<i>smap -Db -c</i>.
The time to boot a bgblock is related to its size, but should range from 
from a few minutes to about 15 minutes for a bgblock containing 64 
base partitions. 
Only after the bgblock is READY will your job's output file be created 
and the script execution begin. 
If the bgblock boot fails, SLURM will attempt to reboot several times 
before draining the associated nodes and aborting the job.</p>

<p>The job will continue to be in a RUNNING state until the bgjob has 
completed and the bgblock ownership is changed. 
The time for completing a bgjob has freqently been on the order of 
five minutes.
In summary, your job may appear in SLURM as RUNNING for 15 minutes 
before the script actually begins to 5 minutes after it completes.
These delays are the result of BG infrastructure issues and are 
not due to anything in SLURM.</p>

<p>When using smap in curses mode you can scroll through the different windows
using the arrow keys.  The <b>up</b> and <b>down</b> arrow keys scroll 
the window containing the grid, and the <b>left</b> and <b>right</b> arrow 
keys scroll the window containing the text information.</p>
 
<p class="footer"><a href="#top">top</a></p>

<h3>System Administration</h3>

<p>As of IBM's REV 2 driver SLURM must be built in 64bit mod.  
This can be done by specifying <i>CFLAGS=-m64 CXX="g++ -m64"</i>.  Both CFLAGS
and CXX must be set for slurm to compile correctly.
<p>Building a Blue Gene compatible system is dependent upon the 
<i>configure</i> program locating some expected files. 
In particular, the configure script searches for <i>libdb2.so</i> in the 
directories <i>/home/bgdb2cli/sqllib</i> and <i>/u/bgdb2cli/sqllib</i>.
If your DB2 library file is in a different location, use the configure 
option <i>--with-db2-dir=PATH</i> to specify the parent directory.
If you have the same version of the operating system on both the 
Service Node (SN) and the Front End Nodes (FEN) then you can configure 
and build one set of files on the SN and install them on both the SN and FEN.
Note that if your FENs lack an installed <i>libdb2.so</i>, an smap 
built on the SN will be unable to execute at all on those nodes (it 
calls BG Bridge APIs, that dynamically load <i>libdb2.so</i> completely 
out of our control).
You can handle this in two different ways. 
One option is to build two versions of smap (in the main SLURM RPM), 
one for the SN and the other for the FENs. 
The second option is to create a dummy <i>libdb2.so</i> on the FENs 
(it can just point to libslurm.so) so that smap can be initiated. 
Smap will discover if <i>libdb2.so</i> is invalid and avoid using 
any BG Bridge function calls, which would fail. 
In either case, all smap functionality will be provided on the FEN 
except for the ability to map SLURM node names to and from 
row/rack/midplane data.</p> 

<p>If you have different versions of the operating system on the SN and FEN
(as was the case for some early system installations), then you will need 
to configure and build two sets of files for installation. 
One set will be for the Service Node (SN), which has direct access to the BG Bridge APIs. 
The second set will be for the Front End Nodes (FEN), whick lack access to the 
Bridge APIs and interact with using Remote Proceedure Calls to the slurmctld daemon.
You should see "#define HAVE_BG 1" and "#define HAVE_FRONT_END 1" in the "config.h" 
file for both the SN and FEN builds. 
You should also see "#define HAVE_BG_FILES 1" in config.h on the SN before 
building SLURM. </p>

<p>The slurmctld daemon should execute on the system's service node.
If an optional backup daemon is used, it must be in some location where 
it is capable of executing BG Bridge APIs.
One slurmd daemon should be configured to execute on one of the front end nodes. 
That one slurmd daemon represents communications channel for every base partition. 
A future release of SLURM will support multiple slurmd daemons on multiple
front end nodes.
You can use the scontrol command to drain individual nodes as desired and 
return them to service. </p>

<p>The slurm.conf (configuration) file needs to have the value of <i>InactiveLimit</i>
set to zero or not specified (it defaults to a value of zero). 
This is because there are no job steps and we don't want to purge jobs prematurely.
The value of <i>SelectType</i> must be set to "select/bluegene" in order to have 
node selection performed using a system aware of the system's topography 
and interfaces. 
The value of <i>SchedulerType</i> should be set to "sched/builtin".
The value of <i>Prolog</i> should be set to the full pathname of a program that 
will delay execution until the bgblock identified by the MPIRUN_PARTITION 
environment variable is ready for use. It is recommended that you construct a script 
that serves this function and calls the supplied program <i>sbin/slurm_prolog</i>.
The value of <i>Epilog</i> should be set to the full pathname of a program that 
will wait until the bgblock identified by the MPIRUN_PARTITION environment
variable is no longer usable by this job. It is recommended that you construct a script
that serves this function and calls the supplied program <i>sbin/slurm_epilog</i>.
The prolog and epilog programs are used to insure proper synchronization 
between the slurmctld daemon, the user job, and MMCS.
A multitude of other functions may also be placed into the prolog and 
epilog as desired (e.g. enabling/disabling user logins, puring file systmes, 
etc.).  Sample prolog and epilog scripts follow. </p>

<pre>
#!/bin/bash
# Sample Blue Gene Prolog script
#
# Wait for bgblock to be ready for this job's use
/usr/sbin/slurm_prolog


#!/bin/bash
# Sample Blue Gene Epilog script
#
# Cancel job to start the termination process for this job
# and release the bgblock
/usr/bin/scancel $SLURM_JOBID
#
# Wait for bgblock to be released from this job's use
/usr/sbin/slurm_epilog
</pre>

<p>Since jobs with different geometries or other characteristics do not interfere 
with each other's scheduling, backfill scheduling is not presently meaningful.
SLURM's builtin scheduler on Blue Gene will sort pending jobs and then attempt 
to schedule all of them in priority order. 
This essentailly functions as if there is a separate queue for each job size.
Note that SLURM does support different partitions with an assortment of 
different scheduling parameters.
For example, SLURM can have defined a partition for full system jobs that 
is enabled to execute jobs only at certain times; while a default partition 
could be configured to execute jobs at other times. 
Jobs could still be queued in a partition that is configured in a DOWN 
state and scheduled to execute when changed to an UP state. 
Nodes can also be moved between slurm partitions either by changing 
the slurm.conf file and restarting the slurmctld daemon or by using 
the scontrol reconfig command. </p>

<p>SLURM node and partition descriptions should make use of the 
<a href="#naming">naming</a> conventions described above. For example,
"NodeName=bg[000x733] NodeAddr=frontend0 NodeHostname=frontend0 Procs=1024". 
Based on the prefix you give to the noderange in the NodeName= variable 
the bgl blocks will be named by such.  Thus this can be anything you want, but
needs to be consitant throughout the slurm.conf file.
Note that the values of both NodeAddr and NodeHostname for all 
128 base partitions is the name of the front end node executing 
the slurmd daemon. 
The NodeName values represent base partitions. 
No computers are actually expected to return a value of "bg000"
in response to the <i>hostname</i> command nor will any attempt 
be made to route message traffic to this address. </p>

<p>While users are unable to initiate SLURM job steps on Blue Gene systems, 
this restriction does not apply to user root or SlurmUser. 
Be advised that the one slurmd supporting all nodes is unable to manage a 
large number of job steps, so this ability should be used only to verify normal 
SLURM operation. 
If large numbers of job steps are initiated by slurmd, expect the daemon to 
fail due to lack of memory or other resources. 
It is best to minimize other work on the front end node executing slurmd
so as to maximize its performance and minimize other risk factors.</p>

<p>Presently the system administrator must explicitly define each of the 
Blue Gene partitions (or bgblocks) available to execute jobs. 
(<b>NOTE:</b> Blue Gene partitions are unrelated to SLURM partitions.)
Jobs must then execute in one of these pre-defined bgblocks. 
This is known as <i>static partitioning</i>. 
Each of these bgblocks are explicitly configured with either a mesh or 
torus interconnect.
They must also not overlap, except for the implicitly defined full-system 
bgblock.
In addition to the normal <i>slurm.conf</i> file, a new 
<i>bluegene.conf</i> configuration file is required with this information.
Put <i>bluegene.conf</i> into the SLURM configuration directory with
<i>slurm.conf</i>.
A sample file is installed in <i>bluegene.conf.example</i>. 
System administrators should use the <i>smap</i> tool to build appropriate 
configuration file for static partitioning. 
Note that <i>smap -Dc</i> can be run without the SLURM daemons 
active to establish the initial configuration.
Note that the defined bgblocks may not overlap (except for the 
full-system bgblock, which is implicitly created).
See the smap man page for more information.
You must insure that the nodes defined in <i>bluegene.conf</i> are 
consistent with those defined in <i>slurm.conf</i>.
Note that the Image and Numpsets values defined in <i>bluegene.conf</i>
are used only when SLURM creates bgblocks.
If previously defined bgblocks are used by SLURM, their configurations 
are not altered.
If you change the bgblock layout, then slurmctld and slurmd should 
both be cold-started (e.g. <b>/etc/init.d/slurm startclean</b>).
If you which to modify the Image and Numpsets values for existing
bgblocks, either modify them manually or destroy the bgblocks
and let SLURM recreate them. 
Note that in addition to the bgblocks defined in blugene.conf, an 
additional bgblock is created containing all resources defined 
all of the other defined bgblocks. 
If you modify the bgblocks, it is recommended that you restart 
both slurmctld and slurmd without preserving state 
(<i>/etc/init.d/slurm startclean</i>).
Note that SLURM wiring decisions are based upon the link-cards 
being interconnected in a specific fashion. 
If your BlueGene system is wired in an unconventional fashion, 
modifications to the file <i>src/partition_allocator/partition_allocator.c</i> 
may be required.
Make use of the SLURM partition mechanism to control access to these 
bgblocks. A sample <i>bluegene.conf</i> file is shown below.
<pre>
###############################################################################
# Global specifications for Blue Gene system
#
# BlrtsImage:     BlrtsImage used for creation of all bgblocks.
# LinuxImage:     LinuxImage used for creation of all bgblocks.
# MloaderImage:   MloaderImage used for creation of all bgblocks.
# RamDiskImage:   RamDiskImage used for creation of all bgblocks.
# Numpsets:       The Numpsets used for creation of all bgblocks 
#                 equals this value multiplied by the number of 
#                 base partitions in the bgblock.
#
# BridgeAPILogFile : Pathname of file in which to write the BG 
#                    Bridge API logs.
# BridgeAPIVerbose:  How verbose the BG Bridge API logs should be
#                    0: Log only error and warning messages
#                    1: Log level 0 and information messasges
#                    2: Log level 1 and basic debug messages
#                    3: Log level 2 and more debug message
#                    4: Log all messages
# 
# NOTE: The bg_serial value is set at configuration time using the 
#       "--with-bg-serial=" option. Its default value is "BGL".
###############################################################################
BlrtsImage=/bgl/BlueLight/ppcfloor/bglsys/bin/rts_hw.rts
LinuxImage=/bgl/BlueLight/ppcfloor/bglsys/bin/zImage.elf
MloaderImage=/bgl/BlueLight/ppcfloor/bglsys/bin/mmcs-mloader.rts
RamDiskImage=/bgl/BlueLight/ppcfloor/bglsys/bin/ramdisk.elf
Numpsets=8
#
BridgeAPILogFile=/var/log/slurm/bridgeapi.log
BridgeAPIVerbose=0

###############################################################################
# Define the static partitions (bgblocks)
#
# Nodes: The base partitions (midplanes) in the bgblock using XYZ coordinates
# Type:  Connection type "MESH" or "TORUS" or "SMALL", default is "TORUS" 
#        Type SMALL will divide a midplane into multiple bgblocks
#        4 bgblocks each containing 128 c-nodes
#        (smaller bgblocks are presently not supported)
#
# IMPORTANT NOTES:
# * Ordering is very important for laying out switch wires.  Please create
#   blocks with smap, and once done don't move the order of blocks
#   created.
# * A bgblock is implicitly created containing all resources on the system
# * Bgblocks must not overlap (except for implicitly created bgblock)
#   This will be the case when smap is used to create a configuration file
# * All Nodes defined here must also be defined in the slurm.conf file
###############################################################################
# LEAVE NEXT LINE AS A COMMENT, Full-system bgblock, implicitly created
# Nodes=[000x001] Type=TORUS       # 1x1x2 = 2 midplanes
###############################################################################
# volume = 1x1x1 = 1
Nodes=[000x000] Type=TORUS         # 1x1x1 =  1 midplane
Nodes=[001x001] Type=SMALL         # 1x1x1 =  1 midplane, in four bgblocks

</pre></p>

<p>The above <i>bluegene.conf</i> file defines multiple bgblocks to be 
created in a single midplane (see the "SMALL" option). 
Note that you can not presently control how many bgblocks are created 
in a midplane, it will be four. 
Using this mechanism, up to eight independent jobs can be executed 
simultaneously on a one-rack Blue Gene system.
If defining bgblocks of <i>Type=SMALL</i>, the SLURM partition 
containing them as defined in <i>slurm.conf</i> must have the 
parameter <i>Shared=force</i> to enable scheduling of multiple 
jobs on what SLURM considers a single node. 
SLURM partitions that do not contain bgblocks of <i>Type=SMALL</i> 
may have the parameter <i>Shared=no</i> for a slight improvement in 
scheduler performance. 
As in all SLURM configuration files, parameters and values 
are case insensitive.</p>

<p>One more thing is required to support SLURM interactions with 
the DB2 database (at least as of the time this was written).
DB2 database access is required by the slurmctld daemon only.
All other SLURM daemons and commands interact with DB2 using 
remote procedure calls, which are processed by slurmctld.
DB2 access is dependent upon the environment variable
<b>BRIDGE_CONFIG_FILE</b>. 
Make sure this is set appropriate before initiating the 
slurmctld daemon. 
If desired, this environment variable and any other logic 
can be executed through the script <i>/etc/sysconfig/slurm</i>, 
which is automatically executed by <i>/etc/init.d/slurm</i> 
prior to initiating the SLURM daemons.</p>

<p>At some time in the future, we expect SLURM to support <i>dynamic 
partitioning</i> in which Blue Gene job partitions are created and destroyed 
as needed to accomodate the workload.
At that time the <i>bluegene.conf</i> configuration file will become obsolete.
Dynamic partition does involve substantial overhead including the 
rebooting of c-nodes and I/O nodes.</p>

<p>When slurmctld is initially started on an idle system, the bgblocks 
already defined in MMCS are read using the BG Bridge APIs. 
If these bgblocks do not correspond to those defined in the bluegene.conf 
file, the old bgblocks with a prefix of "RMP" are destroyed and new ones 
created. 
When a job is scheduled, the appropriate bgblock is identified, 
its user set, and it is booted. 
Node use (virtual or coprocessor) is set from the mpirun command line now,
Slurm has nothing to do with setting the node use.
Subsequent jobs use this same bgblock without rebooting by changing 
the associated user field.
The only time bgblocks should be freed and rebooted, in normal operation,
is when going to or from full-system 
jobs (two or more bgblocks sharing base partitions can not be in a 
ready state at the same time).
When this logic became available at LLNL, approximately 85 percent of 
bgblock boots were eliminated and the overhead of job startup went
from about 24% to about 6% of total job time.
Note that bgblocks will remain in a ready (booted) state when 
the SLURM daemons are stopped. 
This permits SLURM daemon restarts without loss of running jobs 
or rebooting of bgblocks.  </p>

<p>Be aware that SLURM will issue multiple bgblock boot requests as 
needed (e.g. when the boot fails). 
If the bgblock boot requests repeatedly fail, SLURM will configure 
the failing nodes to a DRAINED state so as to avoid continuing 
repeated reboots and the likely failure of user jobs. 
A system administrator should address the problem before returning 
the nodes to service.</p>

<p>If you cold-start slurmctld (<b>/etc/init.d/slurm startclean</b> 
or <b>slurmctld -c</b>) it is recommended that you also cold-start 
the slurmd at the same time. 
Failure to do so may result in errors being reported by both slurmd 
and slurmctld due to bgblocks that previously existed being deleted.</p>

<p>A new tool <b>sfree</b> has also been added to help admins free a BG 
block on request.  
<br>For usage use <b>sfree -u</b> and for help <b>-h</b>.</p>

<h4>Debugging</h4>

<p>All of the testing and debugging guidance provided in 
<a href="quickstart_admin.html"> Quick Start Administrator Guide</a>
apply to Blue Gene systems.
One can start the <b>slurmctld</b> and <b>slurmd</b> in the foreground 
with extensive debugging to establish basic functionality. 
Once runnning in production, the configured <b>SlurmctldLog</b> and 
<b>SlurmdLog</b> files will provide historical system information.
On Blue Gene systems, there is also a <b>BridgeAPILogFile</b> defined 
in <b>bluegene.conf</b> which can be configured to contain detailed 
information about every Bridge API call issued.</p>

<p>Note that slurmcltld log messages of the sort 
<i>Nodes bg[000x133] not responding</i> are indicative of the slurmd 
daemon serving as a front-end to those nodes is not responding (on 
non-Blue Gene systems, the slurmd actaully does run on the compute 
nodes, so the message is more meaningful there). </p>

<p class="footer"><a href="#top">top</a></p></td>

</tr>
<tr> 
<td colspan="3"><hr> <p>For information about this page, contact <a href="mailto:slurm-dev@lists.llnl.gov">slurm-dev@lists.llnl.gov</a>.</p>
<p><a href="http://www.llnl.gov/"><img align=middle src="lll.gif" width="32" height="32" border="0"></a></p>
<p class="footer">UCRL-WEB-213976<br>
Last modified 14 October 2005</p></td>
</tr>
</table>
</td>
 </tr>
</table>
<map name="Map">
<area shape="rect" coords="616,4,762,97" href="../">
<area shape="rect" coords="330,1,468,11" href="http://www.llnl.gov/disclaimer.html">
<area shape="rect" coords="11,23,213,115" href="slurm.html">
</map>
</body>
</html>
