<!--#include virtual="header.txt"-->

<h1>Quick Start User Guide</h1>

<h2>Overview</h2>
<p>The Simple Linux Utility for Resource Management (SLURM) is an open source, 
fault-tolerant, and highly scalable cluster management and job scheduling system 
for large and small Linux clusters. SLURM requires no kernel modifications for 
its operation and is relatively self-contained. As a cluster resource manager, 
SLURM has three key functions. First, it allocates exclusive and/or non-exclusive 
access to resources (compute nodes) to users for some duration of time so they 
can perform work. Second, it provides a framework for starting, executing, and 
monitoring work (normally a parallel job) on the set of allocated nodes. Finally, 
it arbitrates conflicting requests for resources by managing a queue of pending 
work.</p>

<h2>Architecture</h2>
<p>As depicted in Figure 1, SLURM consists of a <b>slurmd</b> daemon running on 
each compute node, a central <b>slurmctld</b> daemon running on a management node 
(with optional fail-over twin), and seven utility programs: <b>srun</b>, 
<b>sbcast</b>, <b>scancel</b>, 
<b>sinfo</b>, <b>srun</b>, <b>smap</b>, <b>squeue</b>, and <b>scontrol</b>.  
All of the commands can run anywhere in the cluster.</p>

<div class="figure">
  <img src="arch.gif" width="600"><br />
  Figure 1. SLURM components
</div>

<p>The entities managed by these SLURM daemons, shown in Figure 2, include <b>nodes</b>, 
the compute resource in SLURM, <b>partitions</b>, which group nodes into logical 
sets, <b>jobs</b>, or allocations of resources assigned to a user for 
a specified amount of time, and <b>job steps</b>, which are sets of (possibly 
parallel) tasks within a job. 
The partitions can be considered job queues, each of which has an assortment of 
constraints such as job size limit, job time limit, users permitted to use it, etc.
Priority-ordered jobs are allocated nodes within a partition until the resources 
(nodes, processors, memory, etc.) within that partition are exhausted. Once 
a job is assigned a set of nodes, the user is able to initiate parallel work in 
the form of job steps in any configuration within the allocation. For instance, 
a single job step may be started that utilizes all nodes allocated to the job, 
or several job steps may independently use a portion of the allocation.</p>

<div class="figure">
  <img src="entities.gif" width="291" height="218"><br />
  Figure 2. SLURM entities
</div>

<p class="footer"><a href="#top">top</a></p>

<h2>Commands</h2>
<p>Man pages exist for all SLURM daemons, commands, and API functions. The command 
option <span class="commandline">--help</span> also provides a brief summary of 
options. Note that the command options are all case insensitive.</p>
<p><span class="commandline"><b>srun</b></span> is used to submit a job for execution, 
allocate resources, attach to an existing allocation, or initiate job steps. Jobs 
can be submitted for immediate or later execution (e.g., batch). <span class="commandline">srun</span> 
has a wide variety of options to specify resource requirements, including: minimum 
and maximum node count, processor count, specific nodes to use or not use, and 
specific node characteristics (so much memory, disk space, certain required features, 
etc.). Besides securing a resource allocation, <span class="commandline">srun</span> 
is used to initiate job steps. These job steps can execute sequentially or in 
parallel on independent or shared nodes within the job's node allocation.</p>

<p><span class="commandline"><b>sbcast</b></span> is used to transfer a file
from local disk to local disk on the nodes allocated to a job. This can be 
used to effectively use diskless compute nodes or provide improved performance 
relative to a shared file system.</p>

<p><span class="commandline"><b>scancel</b></span> is used to cancel a pending 
or running job or job step. It can also be used to send an arbitrary signal to 
all processes associated with a running job or job step.</p>

<p><span class="commandline"><b>scontrol</b></span> is the administrative tool 
used to view and/or modify SLURM state. Note that many <span class="commandline">scontrol</span> 
commands can only be executed as user root.</p>

<p><span class="commandline"><b>sinfo</b></span> reports the state of partitions 
and nodes managed by SLURM. It has a wide variety of filtering, sorting, and formatting 
options.</p>

<p><span class="commandline"><b>squeue</b></span> reports the state of jobs or 
job steps. It has a wide variety of filtering, sorting, and formatting options. 
By default, it reports the running jobs in priority order and then the pending 
jobs in priority order.</p>

<p><span class="commandline"><b>smap</b></span> reports state information for 
jobs, partitions, and nodes managed by SLURM, but graphically displays the 
information to reflect network topology.</p>
<p class="footer"><a href="#top">top</a></p>

<h2>Examples</h2>
<p>Execute <span class="commandline">/bin/hostname</span> on four nodes (<span class="commandline">-N4</span>). 
Include task numbers on the output (<span class="commandline">-l</span>). The 
default partition will be used. One task per node will be used by default. </p>
<pre>
adev0: srun -N4 -l /bin/hostname
0: adev9
1: adev10
2: adev11
3: adev12
</pre> <p>Execute <span class="commandline">/bin/hostname</span> in four 
tasks (<span class="commandline">-n4</span>). Include task numbers on the output 
(<span class="commandline">-l</span>). The default partition will be used. One 
processor per task will be used by default (note that we don't specify a node 
count).</p>
<pre>
adev0: srun -n4 -l /bin/hostname
0: adev9
1: adev9
2: adev10
3: adev10
</pre> <p>Submit the script my.script for later execution (<span class="commandline">-b</span>). 
Explicitly use the nodes adev9 and adev10 (<span class="commandline">-w "adev[9-10]"</span>; 
note the use of a node range expression). One processor per task will be used 
by default. The output will appear in the file my.stdout (<span class="commandline">-o 
my.stdout</span>). By default, one task will be initiated per processor on the 
nodes. Note that my.script contains the command <span class="commandline">/bin/hostname</span> 
that executed on the first node in the allocation (where the script runs) plus 
two job steps initiated using the <span class="commandline">srun</span> command 
and executed sequentially.</p>
<pre>
adev0: cat my.script
#!/bin/sh
/bin/hostname
srun -l /bin/hostname
srun -l /bin/pwd

adev0: srun -w &quot;adev[9-10]&quot; -o my.stdout -b my.script
srun: jobid 469 submitted

adev0: cat my.stdout
adev9
0: adev9
1: adev9
2: adev10
3: adev10
0: /home/jette
1: /home/jette
2: /home/jette
3: /home/jette
</pre>

<p>Submit a job, get its status, and cancel it. </p>
<pre>
adev0: srun -b my.sleeper
srun: jobid 473 submitted

adev0: squeue
  JobId Partition Name     User     St TimeLimit Prio Nodes                        
    473 batch     my.sleep jette    R  UNLIMITED 0.99 adev9 
                       
adev0: scancel 473

adev0: squeue
  JobId Partition Name     User     St TimeLimit Prio Nodes            
</pre>

<p>Get the SLURM partition and node status.</p>
<pre>
adev0: sinfo
PARTITION  NODES STATE     CPUS    MEMORY    TMP_DISK NODES
--------------------------------------------------------------------------------
debug          8 IDLE         2      3448       82306 adev[0-7]
batch          1 DOWN         2      3448       82306 adev8
               7 IDLE         2 3448-3458       82306 adev[9-15]
</pre>
<p class="footer"><a href="#top">top</a></p>

<h2>MPI</h2>
<p>MPI use depends upon the type of MPI being used. 
Instructions for using several varieties of MPI with SLURM are
provided below.</p> 

<p> <a href="http://www.open-mpi.org/"><b>Open MPI</b></a> relies upon
SLURM to allocate resources for the job and then mpirun to initiate the 
tasks. For example:
<pre>
$ srun -n4 -A	# allocates four processors and spawns shell for job
&gt; mpirun -np 4 a.out
&gt; exit          # exits shell spawned by initial srun command
</pre>
Note that any direct use of <span class="commandline">srun</span>
will only launch one task per node when the LAM/MPI plugin is used.
To launch more than one task per node usng the
<span class="commandline">srun</span> command, the <i>--mpi=none</i>
option will be required to explicitly disable the LAM/MPI plugin.</p>

<p> <a href="http://www.quadrics.com/"><b>Quadrics MPI</b></a> relies upon SLURM to 
allocate resources for the job and <span class="commandline">srun</span> 
to initiate the tasks. One would build the MPI program in the normal manner 
then initiate it using a command line of this sort:</p>
<pre>
$ srun [OPTIONS] &lt;program&gt; [program args]
</pre>

<p> <a href="http://www.lam-mpi.org/"><b>LAM/MPI</b></a> relies upon the SLURM 
<span class="commandline">srun</span> command to allocate resources using 
either the <span class="commandline">--allocate</span> or the 
<span class="commandline">--batch</span> option. In either case, specify 
the maximum number of tasks required for the job. Then execute the 
<span class="commandline">lamboot</span> command to start lamd daemons. 
<span class="commandline">lamboot</span> utilizes SLURM's 
<span class="commandline">srun</span> command to launch these daemons. 
Do not directly execute the <span class="commandline">srun</span> command 
to launch LAM/MPI tasks. For example: 
<pre>
$ srun -n16 -A     # allocates 16 processors and spawns shell for job
&gt; lamboot
&gt; mpirun -np 16 foo args
1234 foo running on adev0 (o)
2345 foo running on adev1
etc.
&gt; lamclean
&gt; lamhalt
&gt; exit             # exits shell spawned by initial srun command
</pre>
Note that any direct use of <span class="commandline">srun</span> 
will only launch one task per node when the LAM/MPI plugin is configured
as the default plugin.  To launch more than one task per node usng the 
<span class="commandline">srun</span> command, the <i>--mpi=none</i>
option would be required to explicitly disable the LAM/MPI plugin
if that is the system default.</p>

<p class="footer"><a href="#top">top</a></p>

<p><a href="http://www.hp.com/go/mpi"><b>HP-MPI</b></a> uses the 
<span class="commandline">mpirun</span> command with the <b>-srun</b> 
option to launch jobs. For example:
<pre>
$MPI_ROOT/bin/mpirun -TCP -srun -N8 ./a.out
</pre></p>

<p><a href="http://www-unix.mcs.anl.gov/mpi/mpich2/"><b>MPICH2</b></a> jobs 
are launched using the <b>srun</b> command. Just link your program with 
SLURM's implementation of the PMI library so that tasks can communicate
host and port information at startup. (The system administrator can add
these option to the mpicc and mpif77 commands directly, so the user will not 
need to bother). For example:
<pre>
$ mpicc -L&lt;path_to_slurm_lib&gt; -lpmi ...
$ srun -n20 a.out
</pre>
<b>NOTES:</b>
<ul>
<li>Some MPICH2 functions are not currently supported by the PMI 
libary integrated with SLURM</li>
<li>Set the environment variable <b>PMI_DEBUG</b> to a numeric value 
of 1 or higher for the PMI libary to print debugging information</li>
</ul></p>

<p><a href="http://nowlab.cse.ohio-state.edu/projects/mpi-iba"><b>MVAPICH</b></a>
jobs can be launched directly by <b>srun</b> command.
SLURM's <i>mvapich</i> MPI plugin must be used to establish communications 
between the laucnhed tasks. This can be accomplished either using the SLURM 
configuration parameter <i>MpiDefault=mvapich</i> in <b>slurm.conf</b>
or srun's <i>--mpi=mvapich</i> option.
<pre>
$ mpicc ...
$ srun -n16 --mpi=mvapich a.out
</pre>

<p><a href="http://nowlab.cse.ohio-state.edu/projects/mpi-iba"><b>MVAPICH2</b></a>
jobs can be launched directly by <b>srun</b> command.
SLURM's <i>none</i> MPI plugin must be used to establish communications 
between the laucnhed tasks. This can be accomplished either using the SLURM 
configuration parameter <i>MpiDefault=none</i> in <b>slurm.conf</b> 
or srun's <i>--mpi=none</i> option. The program must also be linked with
SLURM's implementation of the PMI library so that tasks can communicate
host and port information at startup. (The system administrator can add
these option to the mpicc and mpif77 commands directly, so the user will not
need to bother).  <b>Do not use SLURM's MVAPICH plugin for MVAPICH2.</b>
<pre>
$ mpicc -L&lt;path_to_slurm_lib&gt; -lpmi ...
$ srun -n16 --mpi=none a.out
</pre>

<p><a href="http://www.research.ibm.com/bluegene/"><b>BlueGene MPI</b></a> relies 
upon SLURM to create the resource allocation and then uses the native
<span class="commandline">mpirun</span> command to launch tasks. 
Build a job script containing one or more invocations of the 
<span class="commandline">mpirun</span> command. Then submit 
the script to SLURM using <span class="commandline">srun</span>
command with the <b>--batch</b> option. For example:
<pre>
$ srun -N2 --batch my.script
</pre>
Note that the node count specified with the <i>-N</i> option indicates
the base partition count.
See <a href="bluegene.html">BlueGene User and Administrator Guide</a> 
for more information.</p>

<p style="text-align:center;">Last modified 26 May 2006</p>

<!--#include virtual="footer.txt"-->
