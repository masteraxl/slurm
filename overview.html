<html>
<head>
<title>SLURM: Overview</title>
</head>
<body>
<h1>SLURM: Overview</h1>

<p>SLURM is an open-source resource manager designed for Linux Clusters 
of all sizes. 
It was developed by the  collaborative efforts of 
<a href="http://www.llnl.gov/">Lawrence Livermore National Laboratory (LLNL)</a> 
and <a href="http://www.lnxi.com/">Linux NetworX</a>. 
It provides three key functions. 
First it allocates exclusive and/or non-exclusive access to resources 
(computer nodes) to users for some duration of time so they can perform work. 
Second, it provides a framework for starting, executing, and monitoring work 
(typically a parallel job) on a set of allocated nodes. 
Finally, it arbitrates conflicting requests for resources by managing a 
queue of pending work. </p>

<p>SLURM is not a sophisticated batch system, but it does provide 
an Applications Programming Interface (API) for integration 
with external schedulers such as 
<a href="http://supercluster.org/maui">The Maui Scheduler</a>.
While other resources managers do exist, SLURM is unique in 
several respects: 
<ul>
<li>It's source code is freely available under the 
<a href="http://www.gnu.org/licenses/gpl.html">GNU General 
Public License</a>.</li>
<li>It is designed to operate in a heterogeneous cluster with 
up to thousands of nodes.</li>
<li>It is portable; written in C with a GNU <i>autoconf</i> 
configuration engine. While initially written for Linux, other UNIX-like 
operating systems should be easy porting targets. 
A plugin mechanism exists to support various interconnects, 
authentication mechanisms, schedulers, etc.</li>
<li>SLURM is highly tolerant of system failures including failure 
of the node executing its control functions.</li>
<li>It is simple enough for the motivated end user to understand 
its source and add functionality.</li>
</ul> 
</p>

<h2>Architecture</h2>

<p>SLURM has a centralized manager, <i>slurmctld</i>, to monitor 
resources and work. 
There may also be a backup manager to assume those responsibilities 
in the event of failure. 
Each compute server (node) has a <i>slurmd</i> daemon, which can be 
compared to a remote shell: it waits for work, executes that work, 
returns status, and waits for more work. 
User tools include <i>srun</i> to initiate jobs, 
<i>scancel</i> to terminate queued or running jobs, 
<i>sinfo</i> to report system status, and 
<i>squeue</i> to report the status of jobs.
There is also an administrative tool <i>scontrol</i> available to 
monitor and/or modify configuration and state information. 
APIs are available for all functions. </p>
<p align=center>
<img src="arch.png">
</p>

<p>SLURM has a general-purpose plugin mechanism available to easily 
support various infrastructure. 
These plugins presently include:
<ul>
<li>Authentication of communications:  
<a href=http://www.cs.berkeley.edu/~bnc/authd>authd</a>, 
<a href=http://www.llnl.gov/linux/munge/>munge</a>, or none (default). </li>
<li>Job logging: text file or none (default).</li>
<li>Scheduler: 
<a href="http://supercluster.org/maui">The Maui Scheduler</a>, 
backfill, or FIFO (default).</li>
<li>Switch or interconnect: 
<a href="http://www.quadrics.com/">Quadrics</a> Elan3 or Elan4 or 
none (actually means nothing requiring special handling, default).</li>
</ul>
</p>

<h2>Configurability</h2>
Node state monitored include: count of processors, size of real memory, 
size of temporary disk space, and state (UP, DOWN, etc.). 
Additional node information includes weight (preference in being allocated 
work) and features (arbitrary information such as processor speed or type). 
Nodes are grouped into disjoint partitions. 
Partition information includes: name, list of associated nodes, 
state (UP or DOWN), maximum job time limit, maximum node count per job, 
group access list, and shared node access (YES, NO or FORCE). 
Bit maps are used to represent nodes and scheduling decisions can be made 
by performing a small number of comparisons and a series of fast bit map 
manipulations.
A sample (partial) SLURM configuration file follows.
<pre>
# 
# Sample /etc/slurm.conf
#
ControlMachine=linux0001
BackupController=linux0002
#
AuthType=auth/authd
Epilog=/usr/local/slurm/sbin/epilog
HeartbeatInterval=60
PluginDir=/usr/local/slurm/lib
Prolog=/usr/local/slurm/sbin/prolog
SlurmctldPort=7002
SlurmctldTimeout=120
SlurmdPort=7003
SlurmdSpoolDir=/var/tmp/slurmd.spool
SlurmdTimeout=120
StateSaveLocation=/usr/local/slurm/slurm.state
SwitchType=switch/elan
TmpFS=/tmp
#
# Node Configurations
#
NodeName=DEFAULT TmpDisk=16384 State=IDLE
NodeName=lx[0001-0002] State=DRAINED
NodeName=lx[0003-8000] Procs=16 RealMemory=2048 Weight=16
NodeName=lx[8001-9999] Procs=32 RealMemory=4096 Weight=40 Feature=1200MHz
#
# Partition Configurations
#
PartitionName=DEFAULT MaxTime=30 MaxNodes=2
PartitionName=login Nodes=lx[0001-0002] State=DOWN
PartitionName=debug Nodes=lx[0003-0030] State=UP    Default=YES
PartitionName=class Nodes=lx[0031-0040] AllowGroups=students
PartitionName=batch Nodes=lx[0041-9999] MaxTime=UNLIMITED MaxNodes=4096
</pre>

<h2>Status</h2>
SLURM has been deployed on all LLNL Linux clusters having Quadrics Elan 
switches since the summer of 2003. 
This includes IA32 and IA64 clusters having over 1000 nodes. 
Fault-tolerance has been excellent. 
Parallel job performance has also been excellent.
The throughput rate of simple 2000 task jobs across 1000 nodes is over 
12 per minute or under five seconds per job.

<hr>
<a href="http://www.llnl.gov/disclaimer.html">Privacy and Legal Notice</a>
<p>URL = http://www.llnl.gov/linux/slurm/overview.html
<p>UCRL-WEB-149399 REV 1
<p>Last Modified January 7, 2004</p>
<address>Maintained by <a href="mailto:slurm-dev@lists.llnl.gov">
slurm-dev@lists.llnl.gov</a></address>
</body>
</html>
