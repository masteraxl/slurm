RELEASE NOTES FOR SLURM VERSION 2.2
05 February 2010 (through SLURM 2.2.0-pre1+)


IMPORTANT NOTE:
If using the slurmdbd (SLURM DataBase Daemon) you must update this first.
The 2.2 slurmdbd will work with SLURM daemons at version 1.3.0 and above.
You will not need to update all clusters at the same time, but it is very
important to update slurmdbd first and having it running before updating
any other clusters making use of it.  No real harm will come from updating
your systems before the slurmdbd, but they will not talk to each other
until you do.

SLURM can be upgraded from version 2.1 to version 2.2 without loss of jobs or
other state. 


HIGHLIGHTS
==========
* Slurmctld restart/reconfiguration operations have been altered. 
  NOTE: There will be no change in behavior unless partition configuration 
  or node Features/Weight are altered using the scontrol command to differ 
  from the contents of the slurm.conf configuration file.

  Preserve current partition state information plus node Feature and Weight 
  state information after slurmctld receives a SIGHUP signal or is restarted 
  with the -R option. Recreate partition plus node information (except node
  State and Reason) from slurm.conf file after executing "scontrol reconfig" 
  or restarting slurmctld *without* the -R option. 

     OPERATION            ACTION
     slurmctld -R         Recover all job, node and partition state
     slurmctld            Recover job state, recreate node and partition state
     slurmctld -c         Recover no jobs, recreate node and partition state
     SIGHUP to slurmctld  Preserve all job, node and partition state
     scontrol reconfig    Preserve job state, recreate node and partition state

  Old logic preserved node Feature plus partition state after "slurmctld" or 
  "scontrol reconfig" rather than recreating it from slurm.conf. Node Weight 
  was formerly always recreated from slurm.conf. 

CONFIGURATION FILE CHANGES (see "man slurm.conf" for details)
=============================================================
* Added VSizeFactor to enforce virtual memory limits for jobs and job steps as
  a percentage of their real memory allocation.

COMMAND CHANGES (see man pages for details)
===========================================
* sinfo -R now has the user and timestamp in separate fields from the reason.

* Job submission commands (salloc, sbatch and srun) have a new option, 
  --time-min, that permits the job's time limit to be reduced to the extent
  required to start early through backfill scheduling with the minimum value
  as specified.

* scontrol now has the ability to change a job step's time limit.

BLUEGENE SPECIFIC CHANGES
=========================

OTHER CHANGES
=============
* Much functionality has been added to account_storage/pgsql.  The plugin
   is still in a very beta state.

* SLURM's PMI library (for MPICH2) has been modified to properly execute an 
  executable program stand-alone (single MPI task launched without srun).

* Added support for spank_get_item() to get S_STEP_ALLOC_CORES and 
  S_STEP_ALLOC_MEM. Support will remain for S_JOB_ALLOC_CORES and 
  S_JOB_ALLOC_MEM. 

* Changed error mesage from "Requested time limit exceeds partition limit"
  "Requested time limit is invalid (exceeds some limit)". The error can be
  triggered by a time limit exceeding the user/bank limit or the time-min
  exceeding the job or partition's time limit.

API CHANGES
===========
* Changed members of the following structs:
job_info_t
	num_procs -> num_cpus
	job_min_cpus -> pn_min_cpus
	job_min_memory -> pn_min_memory
	job_min_tmp_disk -> pn_min_tmp_disk

job_desc_msg_t
	num_procs -> min_cpus
	job_min_cpus -> pn_min_cpus
	job_min_memory -> pn_min_memory
	job_min_tmp_disk -> pn_min_tmp_disk

* Added the following struct definitions:
job_info_t
	time_min

job_desc_msg_t
	max_cpus
	time_min

node_info_t
	reason_time
	reason_uid

update_node_msg_t
	reason_uid

* Changed the following enums

* Added the following API's
slurm_init_update_step_msg()
slurm_update_step()

* Changed the following API's
