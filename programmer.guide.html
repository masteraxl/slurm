<html>
<head>
<title>SLURM Programmer's Guide</title>
</head>
<body>
<h1>SLURM Programmer's Guide</h1>
<h2>Overview</h2>
Simple Linux Utility for Resource Management (SLURM) is an open source,
fault-tolerant, and highly scalable cluster management and job 
scheduling system for Linux clusters of 
thousands of nodes.  Components include machine status, partition
management, job management, and scheduling modules.  The design also 
includes a scalable, general-purpose communication infrastructure.
SLURM requires no kernel modifications and is relatively self-contained.

<h2>Overview</h2>
There is a description of the components and their interactions available 
in a separate document, <a href="overview.ps">SLURM: Simple Linux Utility 
for Resource Management</a>.


<h2>Common Modules</h2>
This directory contains modules of general use throughout the SLURM code. 
The modules are described below.

<dl>
<dt>list.c
<dd>Module is a general purpose list manager. One can define a 
list, add and delete entries, search for entries, etc. 

<dt>list.h
<dd>Module contains definitions for list.c and documentation for its functions.

<dt>slurm.h
<dd>Definitions for common SLURM data structures and functions.
</dl>


<h2>Scancel Modules</h2>
Scancel is a command to cancel running or pending jobs.


<h2>Slurmadmin Modules</h2>
Slurmadmin is the administrator tool for monitoring and modifying SLURM state.


<h2>Slurmctld Modules</h2>
Slurmctld executes on the control machine and orchestrates SLURM activities 
across the entire cluster including monitoring node and partition state, 
scheduling, job queue management, job dispatching, and switch management. 
The slurmctld modules and their functionality are described below.

<dl>
<dt>bits_bytes.c
<dd>A collection of functions for processing bit maps and strings for parsing.

<dt>controller.c
<dd>Primary SLURM daemon to execute on control machine. 
It manages communications the Partition Manager, Switch Manager, and Job Manager threads.

<dt>node_mgr.c
<dd>Module reads, writes, records, updates, and otherwise 
manages the state information for all nodes (machines) in the 
cluster managed by SLURM. 

<dt>node_scheduler.c
<dd>Selects the nodes to be allocated to pending jobs. This makes extensive use 
of bit maps in representing the nodes. It also considers the locality of nodes 
to improve communications performance.

<dt>partition_mgr.c
<dd>Module reads, writes, records, updates, and otherwise 
manages the state information associated with partitions in the 
cluster managed by SLURM. 

<dt>read_config.c
<dd>Read the SLURM configuration file and use it to build node and 
partition data structures.
</dl>


<h2>Slurmd Modules</h2>
Slurmd executes on each compute node. It initiates and terminates user 
jobs and monitors both system and job state. The slurmd modules and their 
functionality are described below.

<dl>
<dt>get_mach_stat.c
<dd>This module gets the machine's status and configuration. 
This includes: size of real memory, size of temporary disk storage, and 
the number of processors.

<dt>read_proc.c
This module collects job state information including real memory use, 
virtual memory use, and CPU time use.
</dl>

<h2>Design Issues</h2>
Most modules are constructed with a some simple, built-in tests. 
Set declarations for DEBUG_MODULE and DEBUG_SYSTEM  both to 1 near 
the top of the module's code. Then compile and run the test. 
Required input scripts and configuration files for these tests 
will be kept in the "etc" subdirectory and the commands to execute 
the tests are in the "Makefile". In some cases, the module must 
be loaded with some other components. In those cases, the support 
modules should be built with the declaration for DEBUG_MODULE set 
to 0 and for DEBUG_SYSTEM set to 1.
<p>
Many of these modules have been built and tested on a variety of 
Unix computers including Redhat's Linux, IBM's AIX, Sun's Solaris, 
and Compaq's Tru-64. The only module at this time which is operating 
system dependent is Get_Mach_Stat.c.
<p>
The node selection logic allocates nodes to jobs in a fashion which 
makes most sense for a Quadrics switch interconnect. It allocates 
the smallest collection of consecutive nodes that satisfies the 
request (e.g. if there are 32 consecutive nodes and 16 consecutive 
nodes available, a job needing 16 or fewer nodes will be allocated 
those nodes from the 16 node set rather than fragment the 32 node 
set). If the job can not be allocated consecutive nodes, it will 
be allocated the smallest number of consecutive sets (e.g. if there 
are sets of available consecutive nodes of sizes 6, 4, 3, 3, 2, 1, 
and 1 then a request for 10 nodes will always be allocated the 6 
and 4 node sets rather than use the smaller sets).


<h2>Application Program Interface (API)</h2>
All functions described below can be issued from any node in the SLURM cluster. 

<dl>
<dt>int Allocate_Resources(char *Job_Spec);
<dd>Allocate resources for the job with the specification Job_Spec.
This call can only be successfully executed by user <b>root</b>.
Returns -2 if Job_Spec can not be successfully parsed.
Returns -1 if the job can not be initiated given current SLURM configuration. 
Returns 0 if the job can not presently be initiated due to busy nodes.
Returns a SLURM job ID greater than zero.

<dt>Get_Acctg_Info(TBD);
<dd>Return job and system accounting information.
This function has yet to be defined.

<dt>int Deallocate_Resources(int Job_Id);
<dd>Deallocated the resources associated with the specified SLURM Job_Id. 
This call can only be successfully executed by user <b>root</b>. 
If there is an active job associated with this resource allocation, it will 
be terminated.
Returns zero or an error code.
Possible error codes include: TBD.

<dt>int Get_Build_Info(char *Info_Req, char **Build_Info);
<dd>Return SLURM build information. 
Specify the names of configuration parameters requested in the string Info_Req. 
All configuration information is returned if the length of Info_Req is zero.
The keywords and values are returned in the buffer Build_Info using the 
format "keyword=value" with white-space between each pair.
The buffer Build_Info is created or its size changed as needed. 
The application is responsible for setting *Build_Info to NULL initially and executing "free(*Build_Info)" 
when the buffer is no longer needed.
Returns an error code or zero if no error.
Possible error codes include: TBD.

<dt>int Get_Job_Info(time_t *Last_Update, int *Version_Job_Record, struct Job_Record *Job_Info, int *Job_Records);
<dd>Load into the buffer Job_Info the current job state information only if changed since Last_Update. 
The buffer Job_Info is created or its size changed as needed. 
The application is responsible for setting *Job_Info to NULL initially and executing "free(*Job_Info)" 
when the buffer is no longer needed.
The value of Last_Update is set with the time of last update.
The value of Version_Job_Record is set with the version number of the structure format. 
The value of Job_Records is set with the count of records returned.
Version_Job_Record can be checked by the application to insure it is built with the appropriate structure format.
Returns an error code or zero if no error.
Possible error codes include: TBD.

<dt>int Get_Key(int *key);
<dd>Load into the location key the value of an authorization key. 
This key can be used as part of a job specification (see Job_Spec in the Run_Job and 
Will_Job_Run functions) to grant access to partitions with access restrictions. 
This call can only be successfully executed by user <b>root</b>.
The key can only be used once to initiate a job. 
A key that has been issued and not utilized in KEY_TIMEOUT seconds (defined at 
SLURM build time) will be revoked.
Returns an error code or zero if no error.
Possible error codes include: TBD.

<dt>int Get_Node_Info(time_t *Last_Update, int *Version_Node_Record, struct Node_Record *Node_Info, int *Node_Records);
<dd>Load into the buffer Node_Info the current node state information only if changed since Last_Update. 
The buffer Node_Info is created or its size changed as needed. 
The application is responsible for setting *Node_Info to NULL initially and executing "free(*Node_Info)" 
when the buffer is no longer needed.
The value of Last_Update is set with the time of last update.
The value of Version_Node_Record is set with the version number of the structure format. 
The value of Node_Records is set with the count of records returned.
Version_Node_Record can be checked by the application to insure it is built with the appropriate structure format.
Returns an error code or zero if no error.
Possible error codes include: TBD.

<dt>int Get_Part_Info(time_t *Last_Update, int *Version_Part_Record, struct Part_Record *Part_Info, int *Part_Records);
<dd>Load into the buffer Part_Info the current partition state information only if changed since Last_Update. 
The buffer Part_Info is created or its size changed as needed. 
The application is responsible for setting *Part_Info to NULL initially and executing "free(*Node_Info)" 
when the buffer is no longer needed.
The value of Last_Update is set with the time of last update.
The value of Version_Part_Record is set with the version number of the structure format. 
The value of Part_Records is set with the count of records returned.
Version_Part_Record can be checked by the application to insure it is built with the appropriate structure format.
Returns an error code or zero if no error.
Possible error codes include: TBD.

<dt>int Kill_Job(int Job_Id);
<dd>Terminate the specified SLURM job. 
The SIGTERM signal is sent to task zero of the job followed by SIGKILL to all processes KILL_WAIT seconds later. 
KILL_WAIT is specified at SLURM build time.
This command can only be issued by user <b>root</b> or the user whose job is specified by Job_Id. 
The Kill_Job request must succeed in removing the job record and releasing its nodes 
for re-use even if one or more of the nodes allocated to the job is not responding. 
The job will be terminated on that node when it returns to service.
Returns zero or an error code.
Possible error codes include: TBD.

<dt>int NodeBitMap2List(char **NodeList, char *BitMap, time_t BitMapTime);
<dd>Translate the supplied Node BitMap into its List into its equivalent List. 
The calling program must execute free(NodeList[0]) to release allocated 
memory. A time stamp associated with the BitMap is supplied in order to 
invalidate old BitMaps when the nodes defined to SLURM change.
Returns zero or an error code.
Possible error codes include: TBD.

<dt>int NodeList2BitMap(char *NodeList, char **BitMap, time_t *BitMapTime);
<dd>Translate the supplied NodeList string into its equivalent BitMap. 
The calling program must execute free(BitMap[0]) to release allocated 
memory. A time stamp associated with the BitMap is returned in order to 
invalidate old BitMaps when the nodes defined to SLURM change.
Returns zero or an error code.
Possible error codes include: TBD.

<dt>int Reconfigure(char *NodeList);
<dd>The SLURM daemons on the specified nodes will re-read the configuration file. 
NodeList contains a comma separated list of nodes. 
All nodes are reconfigured if NodeList has zero length. 
This command can only be issued by user <b>root</b>.
Returns zero or an error code.
Possible error codes include: TBD.

<dt>int Run_Job(char *Job_Spec);
<dd>Initiate the job with the specification Job_Spec.
Returns -2 if Job_Spec can not be successfully parsed.
Returns -1 if the job can not be initiated given current SLURM configuration. 
Returns 0 if the job can not presently be initiated due to busy nodes.
Returns a SLURM job ID greater than zero if the job is being initiated.

<dt>int Signal_Job(int Job_Id, int Signal);
<dd>Send the specified signal to the specified SLURM job. 
The signal is sent only to task zero of the job. 
This command can only be issued by user <b>root</b> or the user whose job 
is specified by Job_Id.
Returns zero or an error code.
Possible error codes include: TBD.

<dt>int Transfer_Resources(pid_t Pid, int Job_Id);
<dd>Transfer the ownership of resources associated with the specified 
SLURM Job_Id to the indicated process. 
This call can only be successfully executed by user <b>root</b>.
Returns zero or an error code.
Possible error codes include: TBD.

<dt>int Update(char *Config_Spec);
<dd>Update the SLURM configuration per Config_Spec. 
The format of Config_Spec is identical to that of the SLURM configuration file 
as described in the <a href="admin.guide.html">SLURM Administrator's Guide</a>. 
This command can only be issued by user <b>root</b>.
Returns zero or an error code.
Possible error codes include: TBD.

<dt>int Upload(char *NodeList);
<dd>Upload into the SLURM node configuration table actual configuration 
as actually reported by SERVER_DAEMON on each node (memory, CPU count, temporary disk, etc.).
This could be used to establish a baseline configuration rather than 
entering the configurations manually into a file. 
Information from all nodes is uploaded if NodeList has zero length. 
This command can only be issued by user <b>root</b>.
Returns zero or an error code.
Possible error codes include: TBD.

<dt>int Will_Job_Run(char *Job_Spec);
<dd>Determine if a job with the specification Job_Spec can be initiated.
Returns -2 if Job_Spec can not be successfully parsed.
Returns -1 if the job can not be initiated given current SLURM configuration. 
Returns 0 if the job can not presently be initiated due to busy nodes.
Returns 1 if the job can be initiated immediately.
</dl>

<h2>Examples of API Use</h2>
<pre>
    char *Build_Info;
    int Error_Code, i, Job_Id, Signal;
    pid_t Proc_Id;
    time_t Last_Update;
    struct Job_Record  *Job_Info;
    struct Node_Record *Node_Info;
    struct Part_Record *Part_Info;
    int Job_Records, Node_Records, Part_Records;
    int Version_Job_Record, Version_Node_Record, Version_Part_Record;
    int Key;
    char Scratch[128];
    char *BitMap, *Node_List;

    Build_Info = NULL;
    Error_Code = Get_Build_Info("PROLOG", &Build_Info);
    if (Error_Code != 0) 
        printf("Error %d executing Get_Build_Info for PROLOG\n", Error_Code);
    else
	printf("Get_Build_Info for PROLOG returns %s\n", Build_Info[0]);
    Error_Code = Get_Build_Info("", Build_Info);
    if (Error_Code != 0) 
        printf("Error %d executing Get_Build_Info for everything\n", Error_Code);
    else
	printf("Get_Build_Info for everything returns %s\n", Build_Info[0]);
    free(Build_Info[0]);

    Last_Update = (time_t) 0;
    Job_Info = (struct Job_Record *)NULL;
    Error_Code = Get_Job_Info(&Last_Update, &Version_Job_Record, &Job_Info, &Job_Records);
    if (Error_Code != 0) 
        printf("Error %d executing Get_Job_Info\n", Error_Code);
    else if (Version_Job_Record != JOB_STRUCT_VERSION) 
        printf("Get_Job_Info returned version %d, expected version %d\n", Version_Job_Record, JOB_STRUCT_VERSION);
    else {
        printf("Get_Job_Info returned %d records\n", Job_Records);
        for (i=0; i&ltJob_Records; i++) {
            printf("Job_Id=%d\n", Job_Info[i].Job_Id);
        } /* for */
    } /* else */
    free(Job_Info);

    Error_Code = Get_Key(&Key);
    if (Error_Code != 0) 
        printf("Error %d executing Get_Key\n", Error_Code);
    else 
        printf("Get_Key value is %d\n", Key);

    Last_Update = (time_t) 0;
    Node_Info = (struct Node_Info *)NULL;
    Error_Code = Get_Node_Info(&Last_Update, &Version_Node_Record, &Node_Info, &Node_Records);
    if (Error_Code != 0) 
        printf("Error %d executing Get_Node_Info\n", Error_Code);
    else if (Version_Node_Record != NODE_STRUCT_VERSION) 
        printf("Get_Node_Info returned version %d, expected version %d\n", Version_Node_Record, NODE_STRUCT_VERSION);
    else {
        printf("Get_Node_Info returned %d records\n", Node_Records);
        for (i=0; i&ltNode_Records; i++) {
            printf("NodeName=%s\n", Node_Info[i].Name);
        } /* for */
    } /* else */
    free(Node_Info);

    Last_Update = (time_t) 0;
    Part_Info = (struct Job_Record *)NULL;
    Error_Code = Get_Part_Info(&Last_Update, &Version_Part_Record, &Part_Info, &Part_Records);
    if (Error_Code != 0) 
        printf("Error %d executing Get_Part_Info\n", Error_Code);
    else if (Version_Job_Record != JOB_STRUCT_VERSION) 
        printf("Get_Part_Info returned version %d, expected version %d\n", Version_Part_Record, PART_STRUCT_VERSION);
    else {
        printf("Get_Part_Info returned %d records\n", Part_Records);
        /* Format TBD */
    } /* else */
    free(Job_Info);

    printf("Enter SLURM Job_Id of job to be killed: ");
    fgets(Scratch, sizeof(Scratch), stdin);
    Job_Id = atoi(Scratch);
    Error_Code = Kill_Job(Job_Id);
    if (Error_Code != 0) 
        printf("Error %d executing Kill_Job on job %d\n", Error_Code, Job_Id);

    printf("Enter name of node to reconfigure: ");
    fgets(Scratch, sizeof(Scratch), stdin);
    Error_Code = Reconfigure(Scratch);
    if (Error_Code != 0) 
        printf("Error %d executing Reconfigure on node %s\n", Error_Code, Scratch);

    strcpy(Scratch, "lx[01-10]");
    Error_Code = NodeList2BitMap(Scratch, &BitMap, &Last_Update);
    if (Error_Code != 0) 
        printf("Error %d executing NodeList2BitMap on nodes %s\n", Error_Code, Scratch);

    Error_Code = NodeBitMap2List(&NodeList, BitMap, Last_Update);
    if (Error_Code != 0) 
        printf("Error %d executing NodeBitMap2List on nodes %s\n", Error_Code, Scratch);
    else {
	printf("NodeBitMap2List returned %s, expected %s\n", NodeList, Scratch);
	free(BitMap);
	free(NodeList);
    } /* else */

    printf("Enter job specification: ");
    fgets(Scratch, sizeof(Scratch), stdin);
    Error_Code = Will_Job_Run(Scratch);
    if (Error_Code != 0) 
        printf("Error %d executing Will_Job_Run on specification %s\n", Error_Code, Scratch);
    Error_Code = Run_Job(Scratch);
    if (Error_Code != 0) 
        printf("Error %d executing Run_Job on specification %s\n", Error_Code, Scratch);

    Job_Id = Allocate_Resources(Scratch);
    if (Job_Id <= 0) 
        printf("Error %d executing Allocate_Resources on specification %s\n", Error_Code, Scratch);
    else
        printf("Allocate_Resources to Job ID %d with specification %s\n", Error_Code, Job_Id, Scratch);

    printf("Enter process ID of process to be given the allocated resources: ");
    fgets(Scratch, sizeof(Scratch), stdin);
    Proc_Id = atoi(Scratch);
    Error_Code = Transfer_Resources(Proc_Id, Job_Id);
    if (Error_Code != 0) 
        printf("Error %d executing Transfer_Resources on Job ID %d to Proc ID %d\n", Error_Code, Job_Id, Proc_Id);

    Error_Code = Deallocate_Resources(Job_Id);
    if (Error_Code != 0) 
        printf("Error %d executing Deallocate_Resources on Job ID %d\n", Error_Code, Job_Id);

    printf("Enter SLURM Job_Id of job to be signalled: ");
    fgets(Scratch, sizeof(Scratch), stdin);
    Job_Id = atoi(Scratch);
    printf("Enter signal number: ");
    fgets(Scratch, sizeof(Scratch), stdin);
    Signal = atoi(Scratch);
    Error_Code = Signal_Job(Job_Id, Signal);
    if (Error_Code != 0) 
        printf("Error %d executing Signal_Job on job %d and signal %d\n", Error_Code, Job_Id, Signal);

    printf("Enter configuration update specification: ");
    fgets(Scratch, sizeof(Scratch), stdin);
    Error_Code = Update(Scratch);
    if (Error_Code != 0) 
        printf("Error %d executing Update on specification %s\n", Error_Code, Scratch);

    printf("Enter name of node to upload state from: ");
    fgets(Scratch, sizeof(Scratch), stdin);
    Error_Code = Upload(Scratch);
    if (Error_Code != 0) 
        printf("Error %d executing Upload on node %s\n", Error_Code, Scratch);
</pre>

<h2>To Do</h2>
<ul>
<li>How do we interface with TotalView?</li>

<li>The SLURM Job Manager component would be responsible for enforcing 
job time and size limits plus group access controls.</li>

<li>Deadlines: MCR to be built in July 2002, accepted August 2002.</li>

<li>SLURM needs to use switch for timely distribution of executable and 
stdin files.</li>

<li>get_mach_stat.c is quite system dependent. We probably want to 
construct multiple file names containing the system name (e.g. 
Get_Mach_Stat.aix.c, Get_Mach_Stat.linux.c, etc.) and build accordingly.</li>

</ul>

<hr>
URL = http://www-lc.llnl.gov/dctg-lc/slurm/programmer.guide.html
<p>Last Modified March 14, 2002</p>
<address>Maintained by <a href="mailto:slurm-dev@lists.llnl.gov">
slurm-dev@lists.llnl.gov</a></address>
</body>
</html>
