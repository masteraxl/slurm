<html>
<head>
<title>SLURM Programmer's Guide</title>
</head>
<body>
<h1>SLURM Programmer's Guide</h1>
<h2>Overview</h2>
Simple Linux Utility for Resource Management (SLURM) is an open source,
fault-tolerant, and highly scalable cluster management and job 
scheduling system for Linux clusters of 
thousands of nodes.  Components include machine status, partition
management, job management, and scheduling modules.  The design also 
includes a scalable, general-purpose communication infrastructure.
SLURM requires no kernel modifications and is relatively self-contained.

<h2>Overview</h2>
There is a description of the components and their interactions available 
in a separate document, <a href="overview.ps">SLURM: Simple Linux Utility 
for Resource Management</a>.


<h2>Common Modules</h2>
This directory contains modules of general use throughout the SLURM code. 
The modules are described below.

<dl>
<dt>bits_bytes.c
<dd>A collection of functions for processing bit maps and strings for parsing.

<dt>list.c
<dd>Module is a general purpose list manager. One can define a 
list, add and delete entries, search for entries, etc. 

<dt>list.h
<dd>Module contains definitions for list.c and documentation for its functions.

<dt>slurm.h
<dd>Definitions for common SLURM data structures and functions.

<dt>slurmlib.h
<dd>Definitions for SLURM API data structures and functions.
</dl>


<h2>scancel Modules</h2>
scancel is a command to cancel running or pending jobs.

<dl>
<dt>scancel.c
<dd>A command line interface to cancel jobs.
</dl>


<h2>scontrol Modules</h2>
scontrol is the administrator tool for monitoring and modifying SLURM configuration 
and state. It has a command line interface only

<dl>
<dt>scontrol.c
<dd>A command line interface to slurmctld.
</dl>


<h2>slurmctld Modules</h2>
slurmctld executes on the control machine and orchestrates SLURM activities 
across the entire cluster including monitoring node and partition state, 
scheduling, job queue management, job dispatching, and switch management. 
The slurmctld modules and their functionality are described below.

<dl>
<dt>controller.c
<dd>Primary SLURM daemon to execute on control machine. 
It manages communications the Partition Manager, Switch Manager, and Job Manager threads.

<dt>node_mgr.c
<dd>Module reads, writes, records, updates, and otherwise 
manages the state information for all nodes (machines) in the 
cluster managed by SLURM. 

<dt>node_scheduler.c
<dd>Selects the nodes to be allocated to pending jobs. This makes extensive use 
of bit maps in representing the nodes. It also considers the locality of nodes 
to improve communications performance.

<dt>partition_mgr.c
<dd>Module reads, writes, records, updates, and otherwise 
manages the state information associated with partitions in the 
cluster managed by SLURM. 

<dt>read_config.c
<dd>Read the SLURM configuration file and use it to build node and 
partition data structures.
</dl>


<h2>Slurmd Modules</h2>
Slurmd executes on each compute node. It initiates and terminates user 
jobs and monitors both system and job state. The slurmd modules and their 
functionality are described below.

<dl>
<dt>get_mach_stat.c
<dd>This module gets the machine's status and configuration. 
This includes: size of real memory, size of temporary disk storage, and 
the number of processors.

<dt>read_proc.c
<dd>This module collects job state information including real memory use, 
virtual memory use, and CPU time use.
</dl>

<h2>Design Issues</h2>
Most modules are constructed with a some simple, built-in tests. 
Set declarations for DEBUG_MODULE and DEBUG_SYSTEM  both to 1 near 
the top of the module's code. Then compile and run the test. 
Required input scripts and configuration files for these tests 
will be kept in the "etc" subdirectory and the commands to execute 
the tests are in the "Makefile". In some cases, the module must 
be loaded with some other components. In those cases, the support 
modules should be built with the declaration for DEBUG_MODULE set 
to 0 and for DEBUG_SYSTEM set to 1.
<p>
Many of these modules have been built and tested on a variety of 
Unix computers including Redhat's Linux, IBM's AIX, Sun's Solaris, 
and Compaq's Tru-64. The only module at this time which is operating 
system dependent is Get_Mach_Stat.c.
<p>
The node selection logic allocates nodes to jobs in a fashion which 
makes most sense for a Quadrics switch interconnect. It allocates 
the smallest collection of consecutive nodes that satisfies the 
request (e.g. if there are 32 consecutive nodes and 16 consecutive 
nodes available, a job needing 16 or fewer nodes will be allocated 
those nodes from the 16 node set rather than fragment the 32 node 
set). If the job can not be allocated consecutive nodes, it will 
be allocated the smallest number of consecutive sets (e.g. if there 
are sets of available consecutive nodes of sizes 6, 4, 3, 3, 2, 1, 
and 1 then a request for 10 nodes will always be allocated the 6 
and 4 node sets rather than use the smaller sets).


<h2>Application Program Interface (API)</h2>
All functions described below can be issued from any node in the SLURM cluster. 

<dl>
<dt>int Allocate(char *Spec, char **NodeList);
<dd>Allocate nodes for a job with supplied contraints. 
<dd>Input: Spec - Specification of the job's constraints;
<dd>NodeList - Place into which a node list pointer can be placed;
<dd>Output: NodeList - List of allocated nodes;
<dd>Returns 0 if no error, EINVAL if the request is invalid, 
EAGAIN if the request can not be satisfied at present;
<dd>NOTE: Acceptable specifications include: JobName=<name> NodeList=<list>, 
Features=<features>, Groups=<groups>, Partition=<part_name>, Contiguous, 
TotalCPUs=<number>, TotalNodes=<number>, MinCPUs=<number>, 
MinMemory=<number>, MinTmpDisk=<number>, Key=<number>, Shared=<0|1>
<dd>NOTE: The calling function must free the allocated storage at NodeList[0]

<dt>void Free_Build_Info(void);
<dd>Free the build information buffer (if allocated).
<dd>NOTE: Buffer is loaded by Load_Build and used by Load_Build_Name.

<dt>void Free_Node_Info(void);
<dd>Free the node information buffer (if allocated)
<dd>NOTE: Buffer is loaded by Load_Node and used by Load_Node_Name.

<dt>void Free_Part_Info(void);
<dd>Free the partition information buffer (if allocated)
<dd>NOTE: Buffer is loaded by Load_Part and used by Load_Part_Name.

<dt>int Get_Job_Info(TBD);
<dd>Function to be defined.

<dt>int Get_Key(? *key);
<dd>Load into the location key the value of an authorization key. 
<dd>To be defined.

<dt>int Kill_Job(int Job_Id);
<dd>Terminate the specified SLURM job. 
<dd>TBD.

<dt>int Load_Build(void);
<dd>Update the build information buffer for use by info gathering APIs
<dd>Output: Returns 0 if no error, EINVAL if the buffer is invalid, ENOMEM if malloc failure.
<dd>NOTE: Buffer is used by Load_Build_Name and freed by Free_Build_Info.

<dt>int Load_Build_Name(char *Req_Name, char *Next_Name, char *Value);
<dd>Load the state information about the named build parameter
<dd>Input: Req_Name - Name of the parameter for which information is requested
if "", then get info for the first parameter in list
<dd>Next_Name - Location into which the name of the next parameter is 
stored, "" if no more
<dd>Value - Pointer to location into which the information is to be stored
<dd>Output: Req_Name - The parameter's name is stored here
<dd>Next_Name - The name of the next parameter in the list is stored here
<dd>Value - The parameter's state information
<dd>Returns 0 on success, ENOENT if not found, or EINVAL if buffer is bad
<dd>NOTE:  Req_Name, Next_Name, and Value must be declared by caller with have 
length BUILD_SIZE or larger
<dd>NOTE: Buffer is loaded by Load_Build and freed by Free_Build_Info.

<dt>int Load_Node(time_t *Last_Update_Time);
<dd>Load the supplied node information buffer for use by info gathering APIs if
node records have changed since the time specified. 
<dd>Input: Buffer - Pointer to node information buffer
<dd>Buffer_Size - size of Buffer
<dd>Output: Returns 0 if no error, EINVAL if the buffer is invalid, ENOMEM if malloc failure
<dd>NOTE: Buffer is loaded by Load_Node and freed by Free_Node_Info.

<dt>int Load_Node_Config(char *Req_Name, char *Next_Name, int *CPUs, 
int *RealMemory, int *TmpDisk, int *Weight, char *Features,
char *Partition, char *NodeState);
<dd>Load the state information about the named node
<dd>Input: Req_Name - Name of the node for which information is requested
if "", then get info for the first node in list
<dd>Next_Name - Location into which the name of the next node is 
stored, "" if no more
<dd>CPUs, etc. - Pointers into which the information is to be stored
<dd>Output: Next_Name - Name of the next node in the list
<dd>CPUs, etc. - The node's state information
<dd>Returns 0 on success, ENOENT if not found, or EINVAL if buffer is bad
<dd>NOTE:  Req_Name, Next_Name, Partition, and NodeState must be declared by the 
caller and have length MAX_NAME_LEN or larger.
Features must be declared by the caller and have length FEATURE_SIZE or larger
<dd>NOTE: Buffer is loaded by Load_Node and freed by Free_Node_Info.

<dt>int Load_Part(time_t *Last_Update_Time);
<dd>Update the partition information buffer for use by info gathering APIs if 
partition records have changed since the time specified. 
<dd>Input: Last_Update_Time - Pointer to time of last buffer
<dd>Output: Last_Update_Time - Time reset if buffer is updated
<dd>Returns 0 if no error, EINVAL if the buffer is invalid, ENOMEM if malloc failure
<dd>NOTE: Buffer is used by Load_Part_Name and free by Free_Part_Info.

<dt>int Load_Part_Name(char *Req_Name, char *Next_Name, int *MaxTime, int *MaxNodes, 
int *TotalNodes, int *TotalCPUs, int *Key, int *StateUp, int *Shared, int *Default,
char *Nodes, char *AllowGroups);
<dd>Load the state information about the named partition
<dd>Input: Req_Name - Name of the partition for which information is requested
if "", then get info for the first partition in list
<dd>Next_Name - Location into which the name of the next partition is 
stored, "" if no more
<dd>MaxTime, etc. - Pointers into which the information is to be stored
<dd>Output: Req_Name - The partition's name is stored here
<dd>Next_Name - The name of the next partition in the list is stored here
<dd>MaxTime, etc. - The partition's state information
<dd>Returns 0 on success, ENOENT if not found, or EINVAL if buffer is bad
<dd>NOTE:  Req_Name and Next_Name must be declared by caller with have length MAX_NAME_LEN or larger.
<dd>Nodes and AllowGroups must be declared by caller with length of FEATURE_SIZE or larger.
<dd>NOTE: Buffer is loaded by Load_Part and free by Free_Part_Info.

<dt>int Reconfigure(void);
<dd>Request that slurmctld re-read the configuration files
Output: Returns 0 on success, errno otherwise

<dt>int Run_Job(char *Job_Spec);
<dd>Initiate the job with the specification Job_Spec.
<dd>TBD.

<dt>int Signal_Job(int Job_Id, int Signal);
<dd>Send the specified signal to the specified SLURM job. 
<dd>TBD.

<dt>int Transfer_Resources(pid_t Pid, int Job_Id);
<dd>Transfer the ownership of resources associated with the specified 
<dd>TBD.

<dt>int Update(char *Spec);
<dd>Request that slurmctld update its configuration per request
<dd>Input: A line containing configuration information per the configuration file format
<dd>Output: Returns 0 on success, errno otherwise

<dt>int Will_Job_Run(char *Job_Spec);
<dd>TBD.
</dl>

<h2>Examples of API Use</h2>
Please see the source code of scancel, scontrol, squeue, and srun for examples 
of all APIs.

<h2>To Do</h2>
<ul>
<li>How do we interface with TotalView?</li>

<li>Deadlines: MCR to be built in July 2002, accepted August 2002.</li>

</ul>

<hr>
URL = http://www-lc.llnl.gov/dctg-lc/slurm/programmer.guide.html
<p>Last Modified April 12, 2002</p>
<address>Maintained by <a href="mailto:slurm-dev@lists.llnl.gov">
slurm-dev@lists.llnl.gov</a></address>
</body>
</html>
