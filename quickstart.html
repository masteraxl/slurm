<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                        "http://www.w3.org/TR/REC-html40/loose.dtd">

<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="keywords" content="Simple Linux Utility for Resource Management, SLURM, resource management, 
Linux clusters, high-performance computing, Livermore Computing">
<meta name="LLNLRandR" content="UCRL-WEB-204324">
<meta name="LLNLRandRdate" content="27 January 2005">
<meta name="distribution" content="global">
<meta name="description" content="Simple Linux Utility for Resource Management">
<meta name="copyright"
content="This document is copyrighted U.S.
Department of Energy under Contract W-7405-Eng-48">
<meta name="Author" content="Morris Jette">
<meta name="email" content="jette1@llnl.gov">
<meta name="Classification"
content="DOE:DOE Web sites via organizational
structure:Laboratories and Other Field Facilities">
<title>Simple Linux Utility for Resource Management:Quick Start User Guide</title>
<link href="slurmstyles.css" rel="stylesheet" type="text/css">
</head>

<body bgcolor="#000000" text="#000000" leftmargin="0" topmargin="0">
<table width="770" border="0" cellspacing="0" cellpadding="0">
<tr> 
<td><img src="slurm_banner.jpg" width="770" height="145" usemap="#Map" border="0" alt="Simple Linux Utility for Resource Management"></td>
</tr>
</table>
<table width="770" border="0" cellspacing="0" cellpadding="3" bgcolor="#FFFFFF">
<tr> 
<td width="100%"> 
<table width="760" border="0" cellspacing="0" cellpadding="4" align="right">
<tr>
<td valign="top" bgcolor="#000000"><p><img src="spacer.gif" width="110" height="1" alt=""></p>
<p><a href="slurm.html" class="nav" align="center">Home</a></p>
<p><span class="whitetext">About</span><br>
<a href="overview.html" class="nav">Overview</a><br>
<a href="news.html" class="nav">What's New</a><br>
<a href="publications.html" class="nav">Publications</a><br>
<a href="team.html" class="nav">SLURM Team</a></p>
<p><span class="whitetext">Using</span><br>
<a href="documentation.html" class="nav">Documentation</a><br>
<a href="faq.html" class="nav">FAQ</a><br>
<a href="help.html" class="nav">Getting Help</a></p>
<p><span class="whitetext">Installing</span><br>
<a href="platforms.html" class="nav">Platforms</a><br>
<a href="download.html" class="nav">Download</a><br>
<a href="quickstart_admin.html" class="nav">Guide</a></p></td>
<td><img src="spacer.gif" width="10" height="1" alt=""></td>
<td valign="top"><h2>Quick Start User Guide</h2>
<h3>Overview</h3>
<p>The Simple Linux Utility for Resource Management (SLURM) is an open source, 
fault-tolerant, and highly scalable cluster management and job scheduling system 
for large and small Linux clusters. SLURM requires no kernel modifications for 
its operation and is relatively self-contained. As a cluster resource manager, 
SLURM has three key functions. First, it allocates exclusive and/or non-exclusive 
access to resources (compute nodes) to users for some duration of time so they 
can perform work. Second, it provides a framework for starting, executing, and 
monitoring work (normally a parallel job) on the set of allocated nodes. Finally, 
it arbitrates conflicting requests for resources by managing a queue of pending 
work.</p>
<h3>Architecture</h3>
<p>As depicted in Figure 1, SLURM consists of a <b>slurmd</b> daemon running on 
each compute node, a central <b>slurmctld</b> daemon running on a management node 
(with optional fail-over twin), and five command line utilities: <b>srun</b>, 
<b>scancel</b>, <b>sinfo</b>, <b>squeue</b>, and <b>scontrol</b>, which can run 
anywhere in the cluster.</p>
<p><img src="arch.gif" width="552" height="432"> 
<p><b>Figure 1. SLURM components</b></p>
<p>The entities managed by these SLURM daemons, shown in Figure 2, include <b>nodes</b>, 
the compute resource in SLURM, <b>partitions</b>, which group nodes into logical 
disjoint sets, <b>jobs</b>, or allocations of resources assigned to a user for 
a specified amount of time, and <b>job steps</b>, which are sets of (possibly 
parallel) tasks within a job. Priority-ordered jobs are allocated nodes within 
a partition until the resources (nodes) within that partition are exhausted. Once 
a job is assigned a set of nodes, the user is able to initiate parallel work in 
the form of job steps in any configuration within the allocation. For instance, 
a single job step may be started that utilizes all nodes allocated to the job, 
or several job steps may independently use a portion of the allocation.</p>
<p><img src="entities.gif" width="291" height="218"> 
<p><b>Figure 2. SLURM entities</b></p>
<p class="footer"><a href="#top">top</a></p>
<h3>Commands</h3>
<p>Man pages exist for all SLURM daemons, commands, and API functions. The command 
option <span class="commandline">--help</span> also provides a brief summary of 
options. Note that the command options are all case insensitive.</p>
<p><span class="commandline"><b>srun</b></span> is used to submit a job for execution, 
allocate resources, attach to an existing allocation, or initiate job steps. Jobs 
can be submitted for immediate or later execution (e.g., batch). <span class="commandline">srun</span> 
has a wide variety of options to specify resource requirements, including: minimum 
and maximum node count, processor count, specific nodes to use or not use, and 
specific node characteristics (so much memory, disk space, certain required features, 
etc.). Besides securing a resource allocation, <span class="commandline">srun</span> 
is used to initiate job steps. These job steps can execute sequentially or in 
parallel on independent or shared nodes within the job's node allocation.</p>
<p><span class="commandline"><b>scancel</b></span> is used to cancel a pending 
or running job or job step. It can also be used to send an arbitrary signal to 
all processes associated with a running job or job step.</p>
<p><span class="commandline"><b>scontrol</b></span> is the administrative tool 
used to view and/or modify SLURM state. Note that many <span class="commandline">scontrol</span> 
commands can only be executed as user root.</p>
<p><span class="commandline"><b>sinfo</b></span> reports the state of partitions 
and nodes managed by SLURM. It has a wide variety of filtering, sorting, and formatting 
options.</p>
<p><span class="commandline"><b>squeue</b></span> reports the state of jobs or 
job steps. It has a wide variety of filtering, sorting, and formatting options. 
By default, it reports the running jobs in priority order and then the pending 
jobs in priority order.</p>
<p class="footer"><a href="#top">top</a></p>
<h3>Examples</h3>
<p>Execute <span class="commandline">/bin/hostname</span> on four nodes (<span class="commandline">-N4</span>). 
Include task numbers on the output (<span class="commandline">-l</span>). The 
default partition will be used. One task per node will be used by default. </p>
<pre>
adev0: srun -N4 -l /bin/hostname
0: adev9
1: adev10
2: adev11
3: adev12
</pre> <p>Execute <span class="commandline">/bin/hostname</span> in four 
tasks (<span class="commandline">-n4</span>). Include task numbers on the output 
(<span class="commandline">-l</span>). The default partition will be used. One 
processor per task will be used by default (note that we don't specify a node 
count).</p>
<pre>
adev0: srun -n4 -l /bin/hostname
0: adev9
1: adev9
2: adev10
3: adev10
</pre> <p>Submit the script my.script for later execution (<span class="commandline">-b</span>). 
Explicitly use the nodes adev9 and adev10 (<span class="commandline">-w "adev[9-10]"</span>; 
note the use of a node range expression). One processor per task will be used 
by default. The output will appear in the file my.stdout (<span class="commandline">-o 
my.stdout</span>). By default, one task will be initiated per processor on the 
nodes. Note that my.script contains the command <span class="commandline">/bin/hostname</span> 
that executed on the first node in the allocation (where the script runs) plus 
two job steps initiated using the <span class="commandline">srun</span> command 
and executed sequentially.</p>
<pre>
adev0: cat my.script
#!/bin/sh
/bin/hostname
srun -l /bin/hostname
srun -l /bin/pwd

adev0: srun -w &quot;adev[9-10]&quot; -o my.stdout -b my.script
srun: jobid 469 submitted

adev0: cat my.stdout
adev9
0: adev9
1: adev9
2: adev10
3: adev10
0: /home/jette
1: /home/jette
2: /home/jette
3: /home/jette
</pre> <p>Submit a job, get its status, and cancel it. </p>
<pre>
adev0: srun -b my.sleeper
srun: jobid 473 submitted

adev0: squeue
  JobId Partition Name     User     St TimeLim Prio Nodes                        
    473 batch     my.sleep jette    R  UNLIMIT 0.99 adev9 
                       
adev0: scancel 473

adev0: squeue
  JobId Partition Name     User     St TimeLim Prio Nodes            
</pre> <p>Get the SLURM partition and node status.</p>
<pre>
adev0: sinfo
PARTITION  NODES STATE     CPUS    MEMORY    TMP_DISK NODES
--------------------------------------------------------------------------------
debug          8 IDLE         2      3448       82306 adev[0-7]
batch          1 DOWN         2      3448       82306 adev8
               7 IDLE         2 3448-3458       82306 adev[9-15]
</pre> <p class="footer"><a href="#top">top</a></p>
<h3>MPI</h3>
<p>MPI use depends upon the type of MPI being used. 
Instructions for using several varieties of MPI with SLURM are
provided below.</p> 

<p> <a href="http://www.quadrics.com/">Quadrics MPI</a> relies upon SLURM to 
allocate resources for the job and <span class="commandline">srun</span> 
to initiate the tasks. One would build the MPI program in the normal manner 
then initiate it using a command line of this sort:</p>
<p class="commandline"> srun [OPTIONS] &lt;program&gt; [program args]</p>

<p> <a href="http://www.lam-mpi.org/">LAM/MPI</a> relies upon the SLURM 
<span class="commandline">srun</span> command to allocate resources using 
either the <span class="commandline">--allocate</span> or the 
<span class="commandline">--batch</span> option. In either case, specify 
the maximum number of tasks required for the job. Then execute the 
<span class="commandline">lamboot</span> command to start lamd daemons. 
<span class="commandline">lamboot</span> utilizes SLURM's 
<span class="commandline">srun</span> command to launch these daemons. 
Do not directly execute the <span class="commandline">srun</span> command 
to launch LAM/MPI tasks. For example: 
<pre>
adev0: srun -n16 -A     # allocates resources and spawns shell for job
adev0: lamboot
adev0: mpirun -np 16 foo args
1234 foo running on adev0 (o)
2345 foo running on adev1
etc.
adev0: lamclean
adev0: lamhalt
adev0: exit             # exits shell spawned by initial srun command
</pre> <p class="footer"><a href="#top">top</a></p>

<p><a href="http://www.hp.com/go/mpi">HP-MPI</a> uses the 
<span class="commandline">mpirun</span> command with the <b>-srun</b> 
option to launch jobs. For example:
<pre>
$MPI_ROOT/bin/mpirun -TCP -srun -N8 ./a.out
</pre></p>

<p><a href="http://www.research.ibm.com/bluegene/">BlueGene MPI</a> relies 
upon SLURM to create the resource allocation and then uses the native
<span class="commandline">mpirun</span> command to launch tasks. 
Build a job script containing one or more invocations of the 
<span class="commandline">mpirun</span> command. Then submit 
the script to SLURM using <span class="commandline">srun</span>
command with the <b>--batch</b> option. For example:
<pre>
srun -N2 --batch my.script
</pre></p>

</td>
</tr>
<tr> 
<td colspan="3"><hr> <p>For information about this page, contact <a href="mailto:slurm-dev@lists.llnl.gov">slurm-dev@lists.llnl.gov</a>.</p>
<p><a href="http://www.llnl.gov/"><img align=middle src="lll.gif" width="32" height="32" border="0"></a></p>
<p class="footer">UCRL-WEB-207187<br>
Last modified 27 January 2005</p></td>
</tr>
</table>
</td>
 </tr>
</table>
<map name="Map">
<area shape="rect" coords="616,4,762,97" href="../">
<area shape="rect" coords="330,1,468,11" href="http://www.llnl.gov/disclaimer.html">
<area shape="rect" coords="11,23,213,115" href="slurm.html">
</map>
</body>
</html>
