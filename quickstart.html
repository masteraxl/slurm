<html>
<head>
<title>SLURM Quick Start User Guide</title>
</head>
<body>
<h1>SLURM Quick Start User Guide</h1>

<h2>Overview</h2>

The Simple Linux Utility for Resource Management (SLURM) is an open
source, fault-tolerant, and highly scalable cluster management and job
scheduling system for Linux clusters large and small.  SLURM requires
no kernel modifications for it operation and is relatively self-contained.

As a cluster resource manager, SLURM has three key functions.  First,
it allocates exclusive and/or non-exclusive access to resources (compute
nodes) to users for some duration of time so they can perform work.
Second, it provides a framework for starting, executing, and monitoring
work (normally a parallel job) on the set of allocated nodes.  Finally,
it arbitrates conflicting requests for resources by managing a queue of
pending work.

<h2>Architecture</h2>

As depicted in Figure 1, SLURM consists of a <b>slurmd</b> daemon
running on each compute node, a central <b>slurmctld</b> daemon running on
a management node (with optional fail-over twin), and five command line
utilities: <b>srun</b>, <b>scancel</b>, <b>sinfo</b>, <b>squeue</b>, and 
<b>scontrol</b>, which can run anywhere in the cluster. 
<p align=center>
<img src="arch.png">
<p align=center>
Figure 1: SLURM components
</p>
<p>
The entities managed by these SLURM daemons are shown in Figure 2 
and include 
<b>nodes</b>, the compute resource in SLURM, 
<b>partitions</b>, which group nodes into logical disjoint sets,  
<b>jobs</b>, or allocations of resources assigned to a user for a 
specified amount of time, and 
<b>job steps</b>, which are sets of (possibly parallel) tasks within a job.  
Priority-ordered jobs are allocated nodes within a partition until the 
resources (nodes) within that partition are exhausted. 
Once a job is assigned a set of nodes, the user is able to initiate
parallel work in the form of job steps in any configuration within the
allocation. For instance a single job step may be started which utilizes
all nodes allocated to the job, or several job steps may independently 
use a portion of the allocation.
<p align=center>
<img src="entities.png">
<p align=center>
Figure 2: SLURM entities
</p>

<h2>Commands</h2>

Man pages exist for all SLURM daemons, commands, and API functions.
The command option "--help" also provides a brief summary of options.
Note that the command options are all case insensitive.
<p>
<b>srun</b> is used to submit a job for execution, allocate resources, 
attach to an existing allocation, or initiate job steps. 
Jobs can be submitted for immediate or later execution (e.g. batch).
srun has a wide variety of options to specify resource requirements 
including: minimum and maximum node count, processor count, specific 
nodes to use or not use, and specific node characteristics (so much 
memory, disk space, certain required features, etc). 
Besides securing a resource allocation, srun is used to initiate
job steps.
These job steps can execute sequentially or in parallel on independent 
or shared nodes within the job's node allocation.
<p>
<b>scancel</b> is used to cancel a pending or running job or job step. 
It can also be used to send an arbitrary signal to all processes 
associated with a running job or job step.
<p>
<b>scontrol</b> is the administrative tool used to view and/or modify 
SLURM state. 
Note that many scontrol commands can only be executed as user root.
<p>
<b>sinfo</b> reports the state of partitions and nodes managed by SLURM.
It has a wide variety of filtering, sorting, and formatting options. 
<p>
<b>squeue</b> reports the state of jobs or job steps. 
It has a wide variety of filtering, sorting, and formatting options. 
By default, it reports the running jobs in priority order and then the 
pending jobs in priority order.

<h2>Examples</h2>

Execute <i>/bin/hostname</i> on four nodes (<i>-N4</i>). 
Include task numbers on the output (<i>-l</i>).
The default partition will be used. 
One task per node will be used by default.
<pre>
adev0: srun -N4 -l /bin/hostname
0: adev9
1: adev10
2: adev11
3: adev12
</pre>
<p>
Execute <i>/bin/hostname</i> in four tasks (<i>-n4</i>). 
Include task numbers on the output (<i>-l</i>).
The default partition will be used. 
One processor per task will be used by default 
(note that we don't specify a node count).
<pre>
adev0: srun -n4 -l /bin/hostname
0: adev9
1: adev9
2: adev10
3: adev10
</pre>
<p>
Submit the script <i>my.script</i> for later execution (<i>-b</i>).
Explicitly use the nodes adev9 and adev10  (<i>-w "adev[9-10]"</i>, 
note the use of a node range expression). 
One processor per task will be used by default
The output will appear in the file <i>my.stdout</i> (<i>-o my.stdout</i>).
By default, one task will be initiated per processor on the nodes. 
Note that <i>my.script</i> contains the command <i>/bin/hostname</i>
which executed on the first node in the allocation (where the script 
runs) plus two job steps 
initiated using the <i>srun</i> command and executed sequentially.
<pre>
adev0: cat my.script
#!/bin/sh
/bin/hostname
srun -l /bin/hostname
srun -l /bin/pwd

adev0: srun -w "adev[9-10]" -o my.stdout -b my.script
srun: jobid 469 submitted

adev0: cat my.stdout
adev9
0: adev9
1: adev9
2: adev10
3: adev10
0: /home/jette
1: /home/jette
2: /home/jette
3: /home/jette
</pre>
<p>
Submit a job, get its status and cancel it.
<pre>
adev0: srun -b my.sleeper
srun: jobid 473 submitted

adev0: squeue
  JobId Partition Name     User     St TimeLim Prio Nodes                        
    473 batch     my.sleep jette    R  UNLIMIT 0.99 adev9 
                       
adev0: scancel 473

adev0: squeue
  JobId Partition Name     User     St TimeLim Prio Nodes            
</pre>
<p>
Get the SLURM partition and node status.
<pre>
adev0: sinfo
PARTITION  NODES STATE     CPUS    MEMORY    TMP_DISK NODES
--------------------------------------------------------------------------------
debug          8 IDLE         2      3448       82306 adev[0-7]
batch          1 DOWN         2      3448       82306 adev8
               7 IDLE         2 3448-3458       82306 adev[9-15]
</pre>

<h2>MPI</h2>

MPI use depends upon the type of MPI being used. 
<a href="http://www.quadrics.com/">Quadrics</a> MPI relies upon SLURM 
to allocate resources for the job and <i>srun</i> to initiate the tasks. 
One would build the MPI program in the normal manner then initiate it 
using a command line of this sort: <br />
<i>srun [OPTIONS]  &lt;program&gt; [program args]</i>.

<p> <a href="http://www.lam-mpi.org/">LAM/MPI</a> relies upon the SLURM 
<i>srun</i> command to allocate resources using either the <i>--allocate</i> 
or the <i>--batch</i> option. In either case, specify the maximum 
number of tasks required for the job. Then execute the <i>lamboot</i> 
command to start <i>lamd</i> daemons. <i>lamboot</i> utilizes SLURM's
<i>srun</i> command to launch these daemons. Do not directly execute 
the <i>srun</i> command to launch LAM/MPI tasks. For example:
<pre>
adev0: srun -n16 -A     # allocates resources and spawns shell for job
adev0: lamboot
adev0: mpirun -np 16 foo args
1234 foo running on adev0 (o)
2345 foo running on adev1
etc.
adev0: lamclean
adev0: lamhalt
adev0: exit             # exits shell spawned by initial srun command
</pre>

<hr>
<a href="http://www.llnl.gov/disclaimer.html">Privacy and Legal Notice</a>
<p>URL = http://www.llnl.gov/linux/slurm/quickstart.html
<p>UCRL-WEB-201790
<p>Last Modified January 8, 2004</p>
<address>Maintained by <a href="mailto:slurm-dev@lists.llnl.gov">
slurm-dev@lists.llnl.gov</a></address>
</body>
</html>
