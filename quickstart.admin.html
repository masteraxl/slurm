<html>
<head>
<title>SLURM Quick Start Administrator Guide</title>
</head>
<body>
<h1>SLURM Quick Start Administrator Guide</h1>

<h2>Overview</h2>

Please see the <a href="quickstart.html">Quick Start User Guide</a>
for a general overview.

<h2>Daemons</h2>

<b>slurmctld</b> is sometimes called the <i>controller</i> daemon. 
It orchestrates SLURM activities including: queuing of job, 
monitoring node state, and allocating resources (nodes) to jobs. 
There is an optional backup controller that automatically assumes 
control in the event the primary controller fails. 
The primary controller resumes control whenever 
it is restored to service. The controller saves its state to disk 
whenever there is a change. This state can be recovered by the controller 
at startup time. <b>slurmctld</b> would typically execute as a 
special user specifically for this purpose (not user root). 
State changes are saved so that jobs and other state can be 
preserved when slurmctld moves or is restarted.
<p>
The <b>slurmd</b> daemon executes on every compute node.
It resembles a remote shell daemon to export control to SLURM.
Since slurmd initiates and manages user jobs, it must execute as 
the user root.
<p>
<b>slurmctld</b> and/or <b>slurmd</b> should be initiated at node startup time 
per the SLURM configuration.

<h2>Infrastructure</h2>
<p>
All communications between SLURM components are authenticated. 
The authentication infrastructure used is specified in the SLURM 
configuration file and options include: 
<a href="http://www.theether.org/authd/">authd</a>, munged and none.
<p>
Quadrics MPI works directly with SLURM on systems having Quadrics 
interconnects. For non-Quadrics interconnect systems,
<a href="http://www.lam-mpi.org/">LAM/MPI</a> is the preferred 
MPI infrastructure. LAM/MPI uses the command <i>lamboot</i> to 
initiate job-specific daemons on each node using SLURM's 
<i>srun</i> command. This places all MPI processes in a 
process-tree under the control of the <b>slurmd</b> daemon.
LAM/MPI version 7.0.4 or higher contains support for SLURM.
<p>
SLURM's default scheduler is FIFO (First-In First-Out). 
A backfill scheduler plugin is also available. 
Backfill scheduling will initiate a lower-priority job 
if doing so does not delay the expected initiation time 
of higher priority jobs; essentially using smaller jobs 
to fill holes in the resource allocation plan.
<a href="http://supercluster.org/maui">The Maui Scheduler</a> 
offers sophisticated scheduling algorithms to control 
SLURM's workload.
Motivated users can even develop their own scheduler plugin 
if so desired. 
<p>
SLURM uses the syslog function to record events. It uses a 
range of importance levels for these messages. Be certain
that your system's syslog functionality is operational.
<p>
There is no necessity for synchronized clocks on the nodes. 
Events occur either in real-time based upon message traffic. 
However, synchronized clocks will permit easier analysis of 
SLURM logs from multiple nodes.


<h2>Building and Installing</h2>

Basic instructions to build and install SLURM are shown below.
See the <i>INSTALL</i> file for more details. 

<ol>
<li>cd to the directory containing the SLURM source and type 
<i>./configure</i> with appropriate options.
<li>Type <i>make</i> to compile SLURM
<li> Type <i>make install</i> to install the programs, documentation, 
libaries, header files, etc.
</ol>

The most commonly used arguments to the <i>configure</i> command include:
<dl>
<dt>--enable-debug
<dd>Enable debugging of individual modules
<dt>--prefix=<i>PREFIX</i>
<dd>Install architecture-independent files in PREFIX, default 
value is <i>/usr/local</i>
<dt>--sysconfdir=<i>DIR</i>
<dd>Specify location of SLURM configuration file
<dt>--with-totalview
<dd>compile with support for the TotalView debugger (see 
<a href="http://www.etnus.com/">http://www.etnus.com</a>)
</dl>


<h2>Configuration</h2>

The SLURM configuration file includes a wide variety of
parameters.  A full description of the parameters is included in the
<i>slurm.conf</i> man page.  Rather than duplicate that information,
a sample configuration file is shown below.  Any text following a
"#" is considered a comment.  The keywords in the file are not case
sensitive, although the argument typically is (e.g. "SlurmUser=slurm"
might be specified as "slurmuser=slurm").  The control machine, like
all other machine specifications can include both the host name and
the name used for communications.  In this case, the host's name is
"mcri" and the name "emcri" is used for communications. The "e" prefix
identifies this as an ethernet address at this site.  Port numbers to be
used for communications are specified as well as various timer values.
<p>
A description of the nodes and their grouping into non-overlapping 
partitions is required.
Partition and node specifications use node range expressions to identify
nodes in a concise fashion.  This configuration file defines a 1154 node
cluster for SLURM, but might be used for a much larger cluster by just
changing a few node range expressions.
Specify the minimum processor count (<i>Procs</i>), real memory space
(<i>RealMemory</i>, megabytes), and temporary disk space (<i>TmpDisk</i>, 
megabytes) that a node should have to be considered available for use. 
Any node lacking these minimum configuration values will be considered 
<i>DOWN</i> and not scheduled.

<pre>
# 
# Sample /etc/slurm.conf for mcr.llnl.gov
#
ControlMachine=mcri   ControlAddr=emcri 
#
AuthType=auth/authd
Epilog=/usr/local/slurm/etc/epilog
FastSchedule=1
JobCompLoc=/var/tmp/jette/slurm.job.log
JobCompType=jobcomp/filetxt
JobCredPrivateKey=/usr/local/etc/slurm.key
JobCredPublicKey=/usr/local/etc/slurm.cert
PluginDir=/usr/local/slurm/lib/slurm
Prolog=/usr/local/slurm/etc/prolog
SchedulerType=sched/backfill
SlurmUser=slurm
SlurmctldPort=7002
SlurmctldTimeout=300
SlurmdPort=7003
SlurmdSpoolDir=/var/tmp/slurmd.spool
SlurmdTimeout=300
StateSaveLocation=/tmp/slurm.state
SwitchType=switch/elan
#
# Node Configurations
#
NodeName=DEFAULT Procs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN
NodeName=mcr[0-1151]  NodeAddr=emcr[0-1151]
#
# Partition Configurations
#
PartitionName=DEFAULT State=UP    
PartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES
PartitionName=pbatch Nodes=mcr[192-1151] 
</pre>

<p>
You will should create unique job credential keys for your site using 
the program <a href="http://www.openssl.org">openssl</a>. 
An example of how to do this is shown below. 
Specify file names that match the values of JobCredentialPrivateKey and 
JobCredentialPublicCertificate in your configuration file.
The JobCredentialPrivateKey file must be readable only by SlurmUser.
The JobCredentialPublicCertificate file must be readable by all users.
<pre>
openssl genrsa -out /usr/local/etc/slurm.key 1024
openssl rsa -in /usr/local/etc/slurm.key -pubout -out /usr/local/etc/slurm.cert
</pre>

<p>
SLURM does not use reserved ports to authenticate communication
between components. You will need to have at least one "auth"
plugin. Currently, only three authentication plugins are supported:
<i>auth/none</i>, <i>auth/authd</i>, and <i>auth/munge</i>. 
The <i>auth/none</i> plugin is built and used by default, but either 
Brent Chun's <a href="http://www.theether.org/authd/">authd</a>, or Chris
Dunlap's Munge should be installed in order to get properly authenticated
communications.  The configure script in the top-level directory of this
distribution will determine which authentication plugins may be built. 
The configuration file specifies which of the available plugins will 
be utilized.
<p>
A Portable Authentication Manager (PAM) module is available for SLURM 
that can prevent a user from accessing a node which he has not been 
allocated, if that mode of operation is desired.

<h2>Starting the Daemons</h2>

For testing purposes you may want to start by just running 
slurmctld and slurmd on one node. By default, they execute 
in the background. Use the "-D" option for each daemon to 
execute them in the foreground and logging will be done to 
your terminal. The "-v" option will log events in more detail 
with more v's increasing the level of detail (e.g. "-vvvvvv").
You can use one window to execute "slurmctld -D -vvvvvv", 
a second window to execute "slurmd -D -vvvvv", and a third 
window to execute commands such as "srun -N1 /bin/hostname" 
to confirm basic functionality. 
<p>
Another important option for the daemons is "-c" to clear 
previous state information. 
Without the "-c" option, the daemons will restore any previously 
saved state information: node state, job state, etc.
With the "-c" option all previously running jobs will be purged 
and node state will be restored to the values specified in 
the configuration file. 
This means that a node configured down manually using the <i>scontrol</i>
command will be returned to service unless also noted as being down 
in the configuration file. 
In practice, SLURM restarts with preservation consistently.
<p>
A thorough battery of <a href="tests.html">tests</a> written in 
the "expect" language is also available. 

<h2>Administration Examples</h2>

<i>scontrol</i> can be used to print all system information and 
modify most of it. Only a few examples are shown below.
Please see the scontrol man page for full details.
The commands and options are all case insensitive.
<p>
Print detailed state of all jobs in the system.
<pre>
adev0: scontrol
scontrol: show job
JobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED
   Priority=4294901286 Partition=batch BatchFlag=0
   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED
   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59
   NodeList=adev8 NodeListIndecies=-1
   ReqProcs=0 MinNodes=0 Shared=0 Contiguous=0
   MinProcs=0 MinMemory=0 Features=(null) MinTmpDisk=0
   ReqNodeList=(null) ReqNodeListIndecies=-1

JobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING
   Priority=4294901285 Partition=batch BatchFlag=0
   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED
   StartTime=03/19-12:54:01 EndTime=NONE
   NodeList=adev8 NodeListIndecies=8,8,-1
   ReqProcs=0 MinNodes=0 Shared=0 Contiguous=0
   MinProcs=0 MinMemory=0 Features=(null) MinTmpDisk=0
   ReqNodeList=(null) ReqNodeListIndecies=-1
</pre>
<p>
Print the detailed state of job 477 and change its priority to zero. 
A priority of zero prevents a job from being initiated (it is held
in <i>pending</i> state).
<pre>
adev0: scontrol
scontrol: show job 477
JobId=477 UserId=bob(6885) Name=sleep JobState=PENDING
   Priority=4294901286 Partition=batch BatchFlag=0
   <i>more data removed....</i>
scontrol: update JobId=477 Priority=0
</pre>
<p>
Print the state of node <i>adev13</i> and drain it.
To drain a node specify a new state of "DRAIN", "DRAINED", or "DRAINING".
SLURM will automatically set it to the appropriate value of either "DRAINING"
or "DRAINED" depending if the node is allocated or not.
Return it to service later.
<pre>
adev0: scontrol
scontrol: show node adev13
NodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000
   Weight=16 Partition=debug Features=(null) 
scontrol: update NodeName=adev13 State=DRAIN
scontrol: show node adev13
NodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000
   Weight=16 Partition=debug Features=(null) 
scontrol: quit
<i>Later</i>
adev0: scontrol 
scontrol: show node adev13
NodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000
   Weight=16 Partition=debug Features=(null) 
scontrol: update NodeName=adev13 State=IDLE
</pre>
<p>
Reconfigure all slurm daemons on all nodes. 
This should be done after changing the SLURM configuration file.
<pre>
adev0: scontrol reconfig
</pre>
<p>
Print the current slurm configuration. 
This also reports if the primary and secondary controllers (slurmctld 
daemons) are responding. 
To just see the state of the controllers, use the command "ping".
<pre>
adev0: scontrol show config
Configuration data as of 03/19-13:04:12
AuthType          = auth/munge
BackupAddr        = eadevj
BackupController  = adevj
ControlAddr       = eadevi
ControlMachine    = adevi
Epilog            = (null)
FastSchedule      = 1
FirstJobId        = 1
NodeHashBase      = 10
HeartbeatInterval = 60
InactiveLimit     = 0
JobCompLoc        = /var/tmp/jette/slurm.job.log
JobCompType       = jobcomp/filetxt
JobCredPrivateKey = /etc/slurm/slurm.key
JobCredPublicKey  = /etc/slurm/slurm.cert
KillWait          = 30
MaxJobCnt         = 2000
MinJobAge         = 300
PluginDir         = /usr/lib/slurm
Prolog            = (null)
ReturnToService   = 1
SchedulerAuth     = (null)
SchedulerPort     = 65534
SchedulerType     = sched/backfill
SlurmUser         = slurm(97)
SlurmctldDebug    = 4
SlurmctldLogFile  = /tmp/slurmctld.log
SlurmctldPidFile  = /tmp/slurmctld.pid
SlurmctldPort     = 7002 
SlurmctldTimeout  = 300
SlurmdDebug       = 65534
SlurmdLogFile     = /tmp/slurmd.log
SlurmdPidFile     = /tmp/slurmd.pid
SlurmdPort        = 7003
SlurmdSpoolDir    = /tmp/slurmd
SlurmdTimeout     = 300
SLURM_CONFIG_FILE = /etc/slurm/slurm.conf
StateSaveLocation = /usr/local/tmp/slurm/adev
SwitchType        = switch/elan
TmpFS             = /tmp
WaitTime          = 0

Slurmctld(primary/backup) at adevi/adevj are UP/UP
</pre>
<p>
Shutdown all SLURM daemons on all nodes.
<pre>
adev0: scontrol shutdown
</pre>

<hr>
<a href="http://www.llnl.gov/disclaimer.html">Privacy and Legal Notice</a>
<p>URL = http://www.llnl.gov/linux/slurm/quickstart.admin.html
<p>UCRL-WEB-TBD
<p>Last Modified January 8, 2004</p>
<address>Maintained by <a href="mailto:slurm-dev@lists.llnl.gov">
slurm-dev@lists.llnl.gov</a></address>
</body>
</html>
